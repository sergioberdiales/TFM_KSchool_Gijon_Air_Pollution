---
title: "Gijon Air Pollution - Prediction Models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###"Prediction is very difficult, especially if it's about the future."
####--Nils Bohr, Nobel laureate in Physics

###"I have seen the future and it is very much like the present, only longer."
####--Kehlog Albran, The Profit

## PM10 hourly prediction models

We are going to focus on the Constitucion station, which is the only station we have meteorological data. And, at first, we are going to construct models only for predicting PM10 levels. 

As first approach we are going to use auto regressive models. So, we will use as explicative variables only lagged values of the variable to predict. In this case PM10 values. 

Loading packages
```{r , warning= FALSE, message= FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(openair) # http://davidcarslaw.github.io/openair/
library(purrr)
library(lubridate)
library(ggplot2)
library(stringr)
library(knitr)
library(xts)
library(zoo)
library(gridExtra)
library(astsa)
library(rvest)
library(fpp2)
library(ranger)
library(broom)
library(RcppRoll)
library(caret)
```


Data loading
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
air_data_2 <- readRDS("data_rds_files/air_data_2.rds")
constitucion_data <- readRDS("data_rds_files/constitucion_data.rds")

```


### Train, test and validation data

We have 18 years of avalaible data. But we are not going to use all the data. Initially we are going to use just three years for the training of the models (2014-2016) and one year (2017) for testing and validation.

Reasons (pending redaction): 

  - Time of training process. 
  - Not all models can take advantage of such long time series. In fact we will see as sometimes smaller datasets achieve similar results.   

Train data: 2014-01-01 - 2016-12-31

Test data: 2017-01-01 - 2017-09-30

Validation data: 2017-10-01 - 2017-12-31

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

train_201401_201612 <- constitucion_data %>% filter(date_time_utc >= '2014-01-01 00:00:00',
                                       date_time_utc <= '2016-12-31 23:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %>%
                                       rename(PM10_0 = PM10) %>%
                                       ts(frequency = 24)
                                       

test_201701_201709 <- constitucion_data %>% filter(date_time_utc >= '2017-01-01 00:00:00',
                                       date_time_utc <= '2017-09-30 23:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %>%
                                       rename(PM10_0 = PM10) %>%
                                       ts(frequency = 24)


test_lite <- constitucion_data %>% filter(date_time_utc >= '2017-02-01 00:00:00',
                                       date_time_utc <= '2017-02-06 23:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %>%
                                       rename(PM10_0 = PM10) %>%
                                       ts(frequency = 24)

validation_201710_201712 <- constitucion_data %>% filter(date_time_utc >= '2017-10-01 00:00:00',
                                       date_time_utc <= '2017-12-31 23:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %>%
                                       rename(PM10_0 = PM10) %>%
                                       ts(frequency = 24)
```

### Base model. 
In order to create a base model we are going to take as prediction the value from the previous hour. And we begin to forecast just one hour ahead.


```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

# As we don't need any training, because we have already defined the model as Xt = Xt-1, we only need to create a testing dataset. We choose the same period as before but we don't use the time series format (ts).

test_201701_201709_2 <- constitucion_data %>% filter(date_time_utc >= '2017-01-01 00:00:00',
                                       date_time_utc < '2017-10-01 00:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) # We replace the nas (36) by the mean.


base_model <- test_201701_201709_2 %>% mutate(y_pred = lag(PM10, 1)) %>% # We create the column y_pred with the lagged value of PM10 (one hour).
                                     rename(y_test = PM10) %>%         # We change the name of the PM10 column to y_test
                                     na.omit()                         # We remove the observations with nas (just the first row)
                            


RMSE = RMSE(base_model$y_test, base_model$y_pred)
print(RMSE)

MAE(base_model$y_test, base_model$y_pred)
print(MAE)

rss <- sum((base_model$y_pred - base_model$y_test) ^ 2)
tss <- sum((base_model$y_test - mean(base_model$y_test)) ^ 2)
R_squared <- 1 - rss/tss
print(R_squared)
```
So, for the one hour PM10 prediction if we take as prediction the previous PM10 hour level we would have a RMSE of 7.45, a MAE of 5.15 and a R-squared of 0.53. We will take these scores as base references for the one hour prediction models. 

And the standard deviation of the test data is 10.9, which is smaller than the RMSE (7.45). So, this estimation is better than using the mean.
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

sd(base_model$y_test)

```

We take a look to the relation between the predictions and the actual values.
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

plot(base_model$y_pred, base_model$y_test)

```
We plot the distribution of the errors
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

hist(base_model$y_pred - base_model$y_test)
```

We plot two lines with the predictions and the actual values
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
plot(base_model$y_test, type = "l", col = "blue")
lines(base_model$y_pred, type = "l", col = "grey")

```
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

autoplot(ts(base_model$y_test), series="Test data") +
  autolayer(ts(base_model$y_pred),
    series="1-step fitted values")

```
Using the function autoplot from the forecast package we plot the three time series we generated before.
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
autoplot(train_201401_201612)
autoplot(test_201701_201709, ylim = c(0, 400))
autoplot(validation_201710_201712,ylim = c(0, 400))

```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
# model_arima=auto.arima(train_201401_201612,seasonal=TRUE,trace=TRUE) # It took 2 hours to fit the model
#model_arima_0 <- model_arima
#saveRDS(model_arima_0, "data_rds_files/model_arima_0.rds")
model_arima_0 <- readRDS("data_rds_files/model_arima_0.rds")
#plot(forecast(model_arima,h=24))
summary(model_arima_0)

#test$arima=forecast(model_arima,h=24)$mean

```

https://stats.stackexchange.com/questions/223457/predict-from-estimated-arima-model-with-new-data
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

forecast(model_arima_0, 1)

y_train <- forecast(model_arima_0, 1)$fitted
x_train <- train_201401_201612


RMSE = RMSE(x_train, y_train)
print(RMSE)

MAE(x_train, y_train)
print(MAE)

rss <- sum((y_train - x_train) ^ 2)
tss <- sum((x_train - mean(x_train)) ^ 2)
R_squared <- 1 - rss/tss # ?tiene sentido r2 en un modelo arima
print(R_squared)

```


The prediction errors on the test data are smaller. It could be counterintuitive but the test period (2017-01 - 2017-09) contains much less variability (sd = 10.91) than the train period (2014-01 - 2016-12< sd = 14.83). This could be affecting the error scores. 
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

y_pred <- forecast(test_201701_201709, model = model_arima_0, h=1)$fitted
y_test <- test_201701_201709

RMSE = RMSE(y_test, y_pred)
print(RMSE)

MAE(y_test, y_pred)
print(MAE)

rss <- sum((y_pred - y_test) ^ 2)
tss <- sum((y_test - mean(y_test)) ^ 2)
R_squared <- 1 - rss/tss # ?tiene sentido r2 en un modelo arima
print(R_squared)
```
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

sd_train <- sd(train_201401_201612)
print(sd_train)
sd_test <- sd(test_201701_201709)
print(sd_test)
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

autoplot(y_test, series="Test data") +
  autolayer(y_pred,
    series="1-step fitted values")

```









Lo mismo que calcule "a mano"
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

arima.test <- Arima(test_201701_201709, model=model_arima_0)
accuracy(arima.test)
```


```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

#h6 <- fitted(arima.test, h = 6) 
#saveRDS(h6, "data_rds_files/arima_model_0_test_fitted_values_h6.rds")
h6 <- readRDS("data_rds_files/arima_model_0_test_fitted_values_h6.rds")
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
y_test <- test_201701_201709
y_pred <- h6

y_test <- as.data.frame(y_test)
y_pred <- as.data.frame(h6)

df <- bind_cols(y_test, y_pred) %>%
                      na.omit() 

y_test <- df$PM10_0
y_pred <- df$x

RMSE = RMSE(y_test, y_pred)
print(RMSE)

MAE(y_test, y_pred)
print(MAE)

rss <- sum((y_pred - y_test) ^ 2)
tss <- sum((y_test - mean(y_test)) ^ 2)
R_squared <- 1 - rss/tss # ?tiene sentido r2 en un modelo arima
print(R_squared)
```











test lite
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

y_pred <- forecast(test_lite, model = model_arima_0, h=1)$fitted
y_test <- test_lite

RMSE = RMSE(y_test, y_pred)
print(RMSE)

MAE(y_test, y_pred)
print(MAE)

rss <- sum((y_pred - y_test) ^ 2)
tss <- sum((y_test - mean(y_test)) ^ 2)
R_squared <- 1 - rss/tss # ?tiene sentido r2 en un modelo arima
print(R_squared)
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

autoplot(y_test, series="Test data") +
  autolayer(y_pred,
    series="1-step fitted values")

```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

arima.test <- Arima(test_lite, model=model_arima_0)
accuracy(arima.test)
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
h1 <- fitted(arima.test, h = 1) 
h2 <- fitted(arima.test, h = 2)
```



```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
h6 <- fitted(arima.test, h = 6) 

```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

autoplot(y_test, series="Test data") +
  autolayer(h6,
    series="6-step fitted values")

```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

autoplot(y_test, series="Test data") +
  autolayer(h1,
    series="1-step fitted values")

```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
y_test <- test_lite
y_pred <- h6

y_test <- as.data.frame(y_test)
y_pred <- as.data.frame(h6)

df <- bind_cols(y_test, y_pred) %>%
                      na.omit() 

y_test <- df$PM10_0
y_pred <- df$x

RMSE = RMSE(y_test, y_pred)
print(RMSE)

MAE(y_test, y_pred)
print(MAE)

rss <- sum((y_pred - y_test) ^ 2)
tss <- sum((y_test - mean(y_test)) ^ 2)
R_squared <- 1 - rss/tss # ?tiene sentido r2 en un modelo arima
print(R_squared)
```


```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
y_test <- test_lite
y_pred <- h1

y_test <- as.data.frame(y_test)
y_pred <- as.data.frame(h1)

df <- bind_cols(y_test, y_pred) %>%
                      na.omit() 

y_test <- df$PM10_0
y_pred <- df$x

RMSE = RMSE(y_test, y_pred)
print(RMSE)

MAE(y_test, y_pred)
print(MAE)

rss <- sum((y_pred - y_test) ^ 2)
tss <- sum((y_test - mean(y_test)) ^ 2)
R_squared <- 1 - rss/tss # ?tiene sentido r2 en un modelo arima
print(R_squared)
```





Pending: 

  - Poner el mismo maximo en el eje y de los autoplot de los datasets de train, test, validation.
  - Hacer una mejor imputacion de nas. Ahora mismo estoy utilizando la media.
  - Utilizar un periodo de train menor. 









