# Prediction Models. ARIMA

On this notebook we are going to try to predict 
one hour - six hours ahead
with ARIMA
PM10
NO2

## PM10 hourly prediction models

We are going to focus on the Constitucion station, which is the only station w e have meteorological data. And, at first, we are going to construct models only for predicting PM10 levels. 

As first approach we are going to use auto regressive models. So, we will use as explicative variables only lagged values of the variable to predict. In this case PM10 values. 

Loading packages
```{r , warning= FALSE, message= FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(openair) # http://davidcarslaw.github.io/openair/
library(purrr)
library(lubridate)
library(ggplot2)
library(stringr)
library(knitr)
library(xts)
library(zoo)
library(gridExtra)
library(astsa)
library(rvest)
library(fpp2)
library(ranger)
library(broom)
library(RcppRoll)
library(caret)
```


### Data loading
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
air_data_2 <- readRDS("data_rds/air_data_2.rds")
constitucion_data <- air_data_2 %>% filter(station == "1")

```


### Train, test and validation data

We have 18 years of avalaible data. But we are not going to use all the data. Initially we are going to use just three years for the training of the models (2014-2016) and one year (2017) for testing and validation.

Reasons (pending redaction): 

  - Time of training process. 
  - Not all models can take advantage of such long time series. In fact we will see as sometimes smaller datasets achieve similar results.   

Train data: 2014-01-01 - 2016-12-31

Test data: 2017-01-01 - 2017-09-30

Validation data: 2017-10-01 - 2017-12-31

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

train_201401_201612 <- constitucion_data %>% filter(date_time_utc >= '2014-01-01 00:00:00',
                                       date_time_utc <= '2016-12-31 23:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %>% # replacing NAs by the mean.
                                       rename(PM10_0 = PM10) %>%
                                       ts(frequency = 24)

# We generate a smaller training dataset with the last three months of 2016
train_201610_201612 <- constitucion_data %>% filter(date_time_utc >= '2016-10-01 00:00:00',
                                       date_time_utc <= '2016-12-31 23:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %>%
                                       rename(PM10_0 = PM10) %>%
                                       ts(frequency = 24)

                                       

test_201701_201709 <- constitucion_data %>% filter(date_time_utc >= '2017-01-01 00:00:00',
                                       date_time_utc <= '2017-09-30 23:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %>%
                                       rename(PM10_0 = PM10) %>%
                                       ts(frequency = 24)

# We generate a smaller testing dataset with the first two weeks of 2016. 
test_20170101_20170114 <- constitucion_data %>% filter(date_time_utc >= '2017-01-01 00:00:00',
                                       date_time_utc <= '2017-01-14 23:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %>%
                                       rename(PM10_0 = PM10) %>%
                                       ts(frequency = 24)



validation_201710_201712 <- constitucion_data %>% filter(date_time_utc >= '2017-10-01 00:00:00',
                                       date_time_utc <= '2017-12-31 23:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %>%
                                       rename(PM10_0 = PM10) %>%
                                       ts(frequency = 24)
```

### Data Exploration

we plot the three time series using the function autoplot we just generated.
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
autoplot(train_201401_201612)
autoplot(test_201701_201709, ylim = c(0, 400))
autoplot(validation_201710_201712,ylim = c(0, 400))

```
### Base model. 

In order to create a base model we are going to take as prediction the value from the previous hour, forecasting just one hour ahead.

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
# As we don't need any training, because we have already defined the model as Xt = Xt-1, we only need to create a testing dataset. We choose the same period as before but we don't use the time series format (ts).

test_201701_201709_2 <- constitucion_data %>% filter(date_time_utc >= '2017-01-01 00:00:00',
                                       date_time_utc < '2017-10-01 00:00:00') %>%
                                       select(PM10) %>%
                                       mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) # We replace the nas (36) by the mean.


base_model <- test_201701_201709_2 %>% mutate(y_pred = lag(PM10, 1)) %>% # We create the column y_pred with the lagged value of PM10 (one hour).
                                     rename(y_test = PM10) %>%         # We change the name of the PM10 column to y_test
                                     na.omit()                         # We remove the observations with nas (just the first row)
        
y_test <- base_model$y_test
y_pred <- base_model$y_pred

sd_test <- sd(y_test) 

RMSE <- RMSE(y_test, y_pred)

MAE <- MAE(y_test, y_pred)

rss <- sum((y_pred - y_test) ** 2)
tss <- sum((y_test - mean(y_test)) ** 2)
R_squared <- 1 - rss/tss
paste(c("R-Squared:", round(R_squared, 4), "MAE:", round(MAE, 2), "RMSE:", round(RMSE, 2), "Standard Deviation (test data):", round(sd_test, 2)))
```
So, for the one hour PM10 prediction if we take as prediction the previous PM10 hour level we would have a R-squared of 0.5327. So, the model is explaining 53% of the variability of the target variable. 

Estimation of errors. We are going to use the MAE (Mean Absolute Error) in order to compare the different models. We choose the MAE over the RMSE (Root Mean Squared Error) because it is easier to interpret and at the same time it is more robust (less sensitive to outliers). Either way, we are going to calculate the RMSE too, precisely because its sensitivity to outliers. It will give us useful information about the behaviour of each model.

We take a look to the relation between the predictions and the actual values.
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

df <- bind_cols(as.data.frame(y_pred), as.data.frame(y_test))
                
y_pred_y_test_base_model_graph <- ggplot(data = df, aes(x = y_test, y = y_pred)) +
                        geom_point(alpha = 0.5) +
                        theme_minimal()

y_pred_y_test_base_model_graph

```
We plot the distribution of the errors
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}


ggplot(data = df, aes(x = y_pred - y_test)) +
                        geom_histogram() +
                        theme_minimal()

```

We plot two lines with the predictions and the actual values. We have a problem of overplotting.

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

autoplot(ts(base_model$y_pred), series="1-step fitted values") +
  autolayer(ts(base_model$y_test),
    series="Test data") + 
    theme_minimal() +
    theme(legend.position="top")
    

```

We plot just the first 14 days of 2017. 
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

two_weeks <- base_model %>% slice(1:336) # 336 corresponds to 24 hours * 14 days.

autoplot(ts(two_weeks$y_pred), series="1-step fitted values") +
  autolayer(ts(two_weeks$y_test),
    series="Test data") + 
    theme_minimal() +
    theme(legend.position="top")

```

And as expected, we get two identical lines. But one of them, the prediction, one step forward than the line with the actual data. 

### ARIMA model

To fit our first ARIMA model we are going to use the auto.arima function from the Forecast package. This function fits several ARIMA models, with different parameters, and selects the model with the best performance. To do this the function auto.arima uses some approximations to speed up the search of the best model. More info about the algorithm applied [here](https://otexts.org/fpp2/arima-r.html). 

We set the parameter seasonal=TRUE because we want  the function looks for seasonal elements (24 hours). 

We have saved the model fitted as an rds object because of the long time of processing. 
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
# model_arima_0 = auto.arima(train_201401_201612,seasonal=TRUE,trace=TRUE) # It took 2 hours to fit the model
# saveRDS(model_arima_0, "data_rds/model_arima_0.rds")
model_arima_0 <- readRDS("data_rds/model_arima_0.rds")
summary(model_arima_0)


```
EXPLAIN THE RESULTS p, d, q P, D, Q etc...

To obtain the R-Squared...
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

y_train <- forecast(model_arima_0, h = 1)$fitted # We make a forecast one hour ahead and we extract the fitted values from the object forecast.
x_train <- train_201401_201612

rss <- sum((y_train - x_train) ** 2)
tss <- sum((x_train - mean(x_train)) ** 2)
R_squared <- 1 - rss/tss
paste(c("R-Squared:", round(R_squared, 4)))

```


The R-Squared has increased from 0.5327 to 0.6031. But both, RMSE (9.34) and MAE (5.77), are greater than the scores obtained with the Base Model. But we are comparing two different periods, the testing period (Base Model; 2017-01 - 2017-09) versus the training period (ARIMA model; 2014 - 2016).

To make a fair comparison we will have to compare the same periods. Especially since we know they have important differences in variability. 

For this we are going to apply the ARIMA model model_arima_0 on the testing data. Doing so we will be able to get the fitted values from this period and we will be able to obtain its error scores.

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

y_pred <- forecast(test_201701_201709, model = model_arima_0, h=1)$fitted # We call the forecast function passing the testing data through the ARIMA model fitted with the training data and we extract the fitted values.

y_test <- test_201701_201709

sd_test <- sd(y_test) 

RMSE <- RMSE(y_test, y_pred)

MAE <- MAE(y_test, y_pred)

rss <- sum((y_pred - y_test) ** 2)
tss <- sum((y_test - mean(y_test)) ** 2)
R_squared <- 1 - rss/tss
paste(c("R-Squared:", round(R_squared, 4), "MAE:", round(MAE, 2), "RMSE:", round(RMSE, 2), "Standard Deviation (test data):", round(sd_test, 2)))
```

We confirm we are obtaining a better R-Squared: 0.6015 versus the 0.5327 obtained by the Base Model. The errors are sustantially better too. The ARIMA model obtains a MAE of 4.84 (Base Model: 5.15) and a RMSE of 6.88 (Base Model: 7.45). 

Furthermore, the prediction errors on the test data are smaller than in the training data. It could be counterintuitive but the test period (2017-01 - 2017-09) contains much less variability (sd = 10.91) than the training period (2014-01 - 2016-12; sd = 14.83). This could be affecting the error scores. 

We plot the fitted values and the test data.
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

autoplot(y_test, series="Test data") +
  autolayer(y_pred,
    series="1-step fitted values") + 
    theme_minimal() +
    theme(legend.position="top")

```

We have made the forecast for one hour ahead and we have estimated its precission. But what happens when we try to forecast more distant values?

In order to check the precission of this ARIMA model with forecasts more ... we are going to use the fitted 
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

# First of all we create with the Arima function the object arima.test
arima.test <- Arima(test_201701_201709, model=model_arima_0)
```

And with the fitted function, setting its parameter to 6, we fit and extract the fitted values corresponding to the forecast 6 hours ahead (check redaction)
```
h6 <- fitted(arima.test, h = 6) # It takes a lot of time

saveRDS(h6, "data_rds/arima_model_0_test_fitted_values_h6.rds") # We save the final object as a rds file
```
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
fitted_values_h6 <- readRDS("data_rds/arima_model_0_test_fitted_values_h6.rds")
```

We assign to y_test and y_pred the test dataset and its predictions.
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
y_test <- test_201701_201709
y_pred <- fitted_values_h6
```
The first rows of y_pred are NAs. That is because we are making prediction 6 hours ahead. 
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
head(y_pred, 10)
```

So, we remove the NAs from the y_pred time series, and the equivalent rows from the y_test object.
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
y_pred <- y_pred %>% na.omit()
y_test <- y_test %>% subset(start = 8)
```

And we obtain the scores to know the goodness of the model of 6 hours ahead
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

RMSE <- RMSE(y_test, y_pred)

MAE <- MAE(y_test, y_pred)

rss <- sum((y_pred - y_test) ** 2)
tss <- sum((y_test - mean(y_test)) ** 2)
R_squared <- 1 - rss/tss
paste(c("R-Squared:", round(R_squared, 4), "MAE:", round(MAE, 2), "RMSE:", round(RMSE, 2), "Standard Deviation (test data):", round(sd_test, 2)))
```

R-Squared: 0.2118. When we try to predict the levels of PM10 6 hours ahead this Arima model is only able to explain a 21% of the variability.
Errors: Consequentely the errors grow too...


## We have been using a dataset for training of 3 years. Is this necessary? Does the Arima algorithm take advantage of a so long time series?

To check this we are going to use now a training period much shorter. We will use just three months, the last three months of 2016.

We repeat ...
```
model_arima_1 = auto.arima(train_201610_2016_12,seasonal=TRUE,trace=TRUE) # 
saveRDS(model_arima_1, "data_rds/model_arima_1.rds")
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
model_arima_1 <- readRDS("data_rds/model_arima_1.rds")
summary(model_arima_1)
```

The model selected by the auto.arima function is very similar to the former model

train_201610_2016_12:          ARIMA(2,1,4)(0,0,2)[24]
train_201401_201612: ARIMA(2,1,3)(0,0,2)[24] 

Coefficients lite:
         ar1    ar2      ma1      ma2     ma3      ma4    sma1    sma2
      0.1006  0.547  -0.5222  -0.6079  0.1543  -0.0107  0.0977  0.1135


Coefficients 2014-2016:
         ar1     ar2      ma1      ma2     ma3    sma1    sma2
      0.0991  0.5519  -0.5228  -0.6175  0.1642  0.0564  0.0392
      
      
We apply this model to the testing data

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

y_pred <- forecast(test_201701_201709, model = model_arima_1, h=1)$fitted # We call the forecast function passing the testing data through the ARIMA model fitted with the training data and we extract the fitted values.

y_test <- test_201701_201709

sd_test <- sd(y_test) 

RMSE <- RMSE(y_test, y_pred)

MAE <- MAE(y_test, y_pred)

rss <- sum((y_pred - y_test) ** 2)
tss <- sum((y_test - mean(y_test)) ** 2)
R_squared <- 1 - rss/tss
paste(c("R-Squared:", round(R_squared, 4), "MAE:", round(MAE, 2), "RMSE:", round(RMSE, 2), "Standard Deviation (test data):", round(sd_test, 2)))
```

The model scores barely changes. So, in order to speed computation we are going to use this training period from now. If we included seasonal variables as the month or the year we would have to use more long datasets.

test lite
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

y_pred <- forecast(test_20170101_20170114, model = model_arima_0, h=1)$fitted
y_test <- test_20170101_20170114

sd_test <- sd(y_test) 

RMSE <- RMSE(y_test, y_pred)

MAE <- MAE(y_test, y_pred)

rss <- sum((y_pred - y_test) ** 2)
tss <- sum((y_test - mean(y_test)) ** 2)
R_squared <- 1 - rss/tss
paste(c("R-Squared:", round(R_squared, 4), "MAE:", round(MAE, 2), "RMSE:", round(RMSE, 2), "Standard Deviation (test data):", round(sd_test, 2)))
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

autoplot(y_test, series="Test data") +
  autolayer(y_pred,
    series="1-step fitted values") +
    theme(legend.position="top")

```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

arima.test_20170101_20170114 <- Arima(test_20170101_20170114, model=model_arima_1)
accuracy(arima.test)
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
h1_lite <- fitted(arima.test_20170101_20170114, h = 1) 
h2_lite <- fitted(arima.test_20170101_20170114, h = 2)
```



```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
h6_lite <- fitted(arima.test_20170101_20170114, h = 6) 
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

autoplot(y_test, series="Test data") +
  autolayer(h6_lite,
    series="6-step fitted values") +
    theme(legend.position="top")

```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

autoplot(y_test, series="Test data") +
  autolayer(h1_lite,
    series="1-step fitted values") +
    theme(legend.position="top")

```

### NO2 Predictions


We generate the train, test and validations datasets
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

train_201610_201612 <- constitucion_data %>% filter(date_time_utc >= '2016-10-01 00:00:00',
                                       date_time_utc <= '2016-12-31 23:00:00') %>%
                                       select(NO2) %>%
                                       mutate(NO2 = replace_na(NO2, mean(NO2, na.rm = TRUE))) %>%
                                       rename(NO2_0 = NO2) %>%
                                       ts(frequency = 24)

train_201401_201612 <- constitucion_data %>% filter(date_time_utc >= '2014-01-01 00:00:00',
                                       date_time_utc <= '2016-12-31 23:00:00') %>%
                                       select(NO2) %>%
                                       mutate(NO2 = replace_na(NO2, mean(NO2, na.rm = TRUE))) %>% # replacing NAs by the mean.
                                       rename(NO2_0 = NO2) %>%
                                       ts(frequency = 24)

                                       

test_201701_201709 <- constitucion_data %>% filter(date_time_utc >= '2017-01-01 00:00:00',
                                       date_time_utc <= '2017-09-30 23:00:00') %>%
                                       select(NO2) %>%
                                       mutate(NO2 = replace_na(NO2, mean(NO2, na.rm = TRUE))) %>%
                                       rename(NO2_0 = NO2) %>%
                                       ts(frequency = 24)

# We generate a testing dataset with the first two weeks of 2016. 
test_20170101_20170114 <- constitucion_data %>% filter(date_time_utc >= '2017-01-01 00:00:00',
                                       date_time_utc <= '2017-01-14 23:00:00') %>%
                                       select(NO2) %>%
                                       mutate(NO2 = replace_na(NO2, mean(NO2, na.rm = TRUE))) %>%
                                       rename(NO2_0 = NO2) %>%
                                       ts(frequency = 24)



validation_201710_201712 <- constitucion_data %>% filter(date_time_utc >= '2017-10-01 00:00:00',
                                       date_time_utc <= '2017-12-31 23:00:00') %>%
                                       select(NO2) %>%
                                       mutate(NO2 = replace_na(NO2, mean(NO2, na.rm = TRUE))) %>%
                                       rename(NO2_0 = NO2) %>%
                                       ts(frequency = 24)
```

This time we are going straight to create an ARIMA model.

### ARIMA model

Training period: 201401_201612
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
NO2_model_arima_0 = auto.arima(train_201401_201612,seasonal=TRUE,trace=TRUE) 
saveRDS(NO2_model_arima_0, "data_rds/NO2_model_arima_0.rds")
```


```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

y_pred <- forecast(test_201701_201709, model = NO2_model_arima_0, h=1)$fitted # We call the forecast function passing the testing data through the ARIMA model fitted with the training data and we extract the fitted values.

y_test <- test_201701_201709

sd_test <- sd(y_test) 

RMSE <- RMSE(y_test, y_pred)

MAE <- MAE(y_test, y_pred)

rss <- sum((y_pred - y_test) ** 2)
tss <- sum((y_test - mean(y_test)) ** 2)
R_squared <- 1 - rss/tss
paste(c("R-Squared:", round(R_squared, 4), "MAE:", round(MAE, 2), "RMSE:", round(RMSE, 2), "Standard Deviation (test data):", round(sd_test, 2)))
```


Training period 201610_201612
```
NO2_model_arima_1 = auto.arima(train_201610_2016_12,seasonal=TRUE,trace=TRUE) 
saveRDS(NO2_model_arima_1, "data_rds/NO2_model_arima_1.rds")
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
NO2_model_arima_1 <- readRDS("data_rds/NO2_model_arima_1.rds")
summary(NO2_model_arima_1)

```

We apply this model to the testing data

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

y_pred <- forecast(test_201701_201709, model = NO2_model_arima_1, h=1)$fitted # We call the forecast function passing the testing data through the ARIMA model fitted with the training data and we extract the fitted values.

y_test <- test_201701_201709

sd_test <- sd(y_test) 

RMSE <- RMSE(y_test, y_pred)

MAE <- MAE(y_test, y_pred)

rss <- sum((y_pred - y_test) ** 2)
tss <- sum((y_test - mean(y_test)) ** 2)
R_squared <- 1 - rss/tss
paste(c("R-Squared:", round(R_squared, 4), "MAE:", round(MAE, 2), "RMSE:", round(RMSE, 2), "Standard Deviation (test data):", round(sd_test, 2)))
```
Better R-Squared than the PM10 model.
But worse errors scores. Different units. Maybe we can't compare.
More variability. 

Explain this.
Include graphics.






