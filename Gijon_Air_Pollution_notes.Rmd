---
title: "Gijon_Air_Pollution"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Project goals:

  - Construct a web dashboard to visualize the evolution of the main air pollution indicators of Gijon city. (Asturias; Spain).
  - Build a model to predict in the short term (<24h) the evolution of air pollutants concentration by monitoring station.

### Data gathering

Loading packages
```{r , warning= FALSE, message= FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(openair) # http://davidcarslaw.github.io/openair/
library(purrr)
library(lubridate)
library(ggplot2)
library(stringr)
library(knitr)
library(xts)
library(zoo)
library(gridExtra)

```


First of all I have to check if I will have the basic data to make the analysis. I need air pollution and weather data of the Gijon area. 
The town hall of Gijon has an open data web portal here https://transparencia.gijon.es/. We can download pollution air data on csv format from year 2000 to 2017 [here](https://transparencia.gijon.es/search/risp_dataset/page/1808-catalogo-de-datos?utf8=%E2%9C%93&search=aire+&search_sector=&search_format=&commit=Buscar&authenticity_token=j8%2F3CvCuPcDkrRe%2F1NR5RBp0t%2FOOosiA7724w3T2mB4%3D): 

I downloaded 18 csv files with air pollution and weather data of Gijón from years 2000 to 2017. I saved them in the "data" folder.
I downloaded two more files from this web, a csv file with the description of the variables and another csv file with information about the measurement stations. 

We take a look to the information included in  the stations_info.csv file. It includes the stations addresses, longitude, latitude and their IDs and names. All this information, as we will see, is included in the csv files with pollution and weather data too. So, we are not going to use this file anymore.


```{r , warning= FALSE, message= FALSE}
stations <- read_delim('data/stations_info.csv',
                       delim = ';',
                       escape_double = FALSE, 
                       trim_ws = TRUE,
                       locale = locale(encoding = "ISO-8859-1"),
                       col_types = cols(.default = "c"))
stations



```

We can see on this image the location of each station. http://movil.asturias.es/medioambiente/articulos/ficheros/Informe%20de%20calidad%20del%20aire%20en%20Asturias%202016.pdf

![Image source: "Informe de calidad del aire del Principado de Asturias (2016)".](imgs/gijon_stations_map.jpg)

The air_data_descriptors.csv file contains information about the nature of the elements monitored by the stations. Names, descriptions and units.
```{r , warning= FALSE, message= FALSE}
variables <- read_csv('data/air_data_descriptors.csv', locale = locale(encoding = "ISO-8859-1"))
variables

```

In order to import the data from the 18 csv files we list all the files in the object data_files. 

```{r warning= FALSE, message= FALSE}
data_files <- list.files(path = "data", pattern = "air_data_20*")
```

Then, we map the function read_csv on this list in order to import every file and finally merge them in a unique dataframe (air_data_0) with reduce(rbind). 

```{r warning= FALSE, message= FALSE}
air_data_0 <- data_files %>% 
    map(function(x) {
        read_csv(paste0("./data/", x), locale = locale(encoding = "ISO-8859-1"), col_types = cols(.default = "c"))
    }) %>%
    reduce(rbind)
```
We take a look to the data set
```{r warning= FALSE, message= FALSE}
glimpse(air_data_0)
```


We change the names of some variables.

```{r warning= FALSE, message= FALSE}

# Variables names changing
air_data_1 <- air_data_0 %>% rename(station = `Estación`,
                                    station_name = `Título`,
                                    date_time_utc = `Fecha Solar (UTC)`,
                                    latitude = latitud,
                                    longitude = longitud)
```

We imported all the columns as characters in order to avoid problems with the format attributions. So, we have to make now some format variable changes.

We change the date_time_utc format from character to date time. 
```{r warning= FALSE, message= FALSE}

air_data_1$date_time_utc <- ymd_hms(air_data_1$date_time_utc)
```

We change the station and station_name formats from character to factor.
```{r warning= FALSE, message= FALSE}
air_data_1$station <- as.factor(air_data_1$station)
air_data_1$station_name <- as.factor(air_data_1$station_name)
```

We create a vector with all the variables we want to be numeric
```{r warning= FALSE, message= FALSE}
num <- colnames(air_data_1)[c(3, 4, 6:22)]
```
We make the conversion of this set of variables to numeric 
```{r warning= FALSE, message= FALSE}
air_data_1 <- air_data_1 %>% mutate_at(num, as.numeric)
```

We create a dictionary with an alias for each station in order to add a new variable with more
convenient station names
```{r warning= FALSE, message= FALSE}
alias_dict <- data.frame(
      station = c("1", "2", "3", "4", "10", "11"),
      station_alias = c("Constitución", "Argentina", "H. Felgueroso", "Castilla", "Montevil", "Santa Bárbara")
)
```
We join the alias dictionary to the air_data_1 data frame to add the new variable to the data 
set.
```{r warning= FALSE, message= FALSE}
air_data_1 <- air_data_1 %>% left_join(alias_dict, by = 'station')
```

We call the summary function to inspect the data main indicators
```{r warning= FALSE, message= FALSE}
summary(air_data_1)
```

There are several variables which minimun values are -9999.

```{r warning = FALSE, message = FALSE}

kable(air_data_1 %>% filter(SO2 == -9999))


```

They are all from the same day (2000-01-27) and from the same station ('H. Felgueroso'). All the variables from that day, excepts the 'CO' indicator, are equal to '-9999'. 

So, we replace these values by NAs. 
```{r warning = FALSE, message = FALSE}

air_data_2 <- air_data_1 %>% mutate(SO2 = replace(SO2, SO2 == -9999, NA),
                                    NO = replace(NO, NO == -9999, NA),
                                    NO2 = replace(NO2, NO2 == -9999, NA),
                                    PM10 = replace(PM10, PM10 == -9999, NA),
                                    O3 = replace(O3, O3 == -9999, NA))

```


We check again the output of summary. 

```{r warning = FALSE, message = FALSE}
summary(air_data_2)
```

Some pollutant variables have as minimum negative values. It does not make much sense. We take a look to the data in order to quantify the problem.

30 SO2 observations between 2015-12-25 and 2015-12-28 from the Montevil station:

```{r warning = FALSE, message = FALSE}
kable(neg_SO2 <- air_data_2 %>% filter(SO2 < 0))
```

2 RS observations from the Constitucion station:

```{r warning = FALSE, message = FALSE}
kable(neg_RS <- air_data_2 %>% filter(RS < 0))
```

27 TOL observations between the 2008-12-11 and the 2008-12-15 from the Constitucion station:

```{r warning = FALSE, message = FALSE}
kable(neg_TOL <- air_data_2 %>% filter(TOL < 0))
```

59 MXIL observations between the 2008-12-10 and the 2008-12-15 from the Constitucion station:

```{r warning = FALSE, message = FALSE}
kable(neg_MXIL <- air_data_2 %>% filter(MXIL < 0))
```

There are not many cases. We replace them all by NAs and call again the summary function (to do: ask to the data owner about this detail).

```{r warning = FALSE, message = FALSE}

air_data_2 <- air_data_2 %>% mutate(SO2 = replace(SO2, SO2 < 0, NA),
                                    RS = replace(RS, RS < 0, NA),
                                    TOL = replace(TOL, TOL < 0, NA),
                                    MXIL = replace(MXIL, MXIL < 0, NA))

summary(air_data_2)

```

We take a look to the data completeness. What proportion of nas do we have by variable, station, year, etc?
```{r warning = FALSE, message = FALSE}

data_completeness <- air_data_2 %>% 
  group_by(station_alias, year = year(date_time_utc)) %>% 
  summarise_all(funs(round(sum(!is.na(.))/n(), 2))) %>%
  select(-c(3:7, 25:28)) # These columns do not have any na. We exclude them.

kable(head(data_completeness, 10))

```

We are going to check the data completeness by station:

Constitución: There is data registered from the variables SO2, NO, NO2, CO, PM10, 03, dd, vv, TMP,  HR, PRB, HS and LL since the year 2000.
There are measurements of the variables BEN, TOL and MXIL since the year 2006 (only 0.01% ). The PM25 particles are monitored since the year 2008 (2008: only covered 0,02% of the year).
During the year 2008 the completeness of several variables (HR, PRB, HS, LL, BEN, TOL y MXIL) decrease until 88% (to do: check there was not caused by a data importing problem.)
```{r warning = FALSE, message = FALSE}
constitucion_data <- data_completeness %>% filter(station_alias == 'Constitución')
kable(constitucion_data)
```

Argentina: data since the year 2000. Variables: SO2, NO, NO2, CO, PM10 and 03.
```{r warning = FALSE, message = FALSE}
argentina_data <- data_completeness %>% filter(station_alias == 'Argentina')
kable(argentina_data)
```

H. Felgueroso: data since the year 2000. Variables: SO2, NO, NO2, CO, PM10 and 03. During the year 2006 the completeness of the data decrease until 88% (to do: check there was not caused by a data importing problem.)
```{r warning = FALSE, message = FALSE}
felgueroso_data <- data_completeness %>% filter(station_alias == 'H. Felgueroso')
kable(felgueroso_data)
```

Castilla: data since the year 2000. Variables: SO2, NO, NO2, CO, PM10 and 03. During the year 2015 the completeness of the data decrease until 77% (to do: check there was not caused by a data importing problem.)
```{r warning = FALSE, message = FALSE}
castilla_data <- data_completeness %>% filter(station_alias == 'Castilla')
kable(castilla_data)
```

Montevil: Data since the year 2009. Variables: SO2, NO, NO2, 03, dd, vv, TMP, HR, PRB, HS, 
LL and PM25.
```{r warning = FALSE, message = FALSE}
montevil_data <- data_completeness %>% filter(station_alias == 'Montevil')
kable(montevil_data)
```

Santa Bárbara: Data since the year 2016. Variables: NO, NO2, CO, PM10, 03 and PM25
```{r warning = FALSE, message = FALSE}
barbara_data <- data_completeness %>% filter(station_alias == 'Santa Bárbara')
kable(barbara_data)
```

All the stations have 2018 data, but it is just 6 observations. We drop them to avoid problems when visualising the data.

```{r warning = FALSE, message = FALSE}
observations_per_year <- air_data_2 %>% group_by(year = year(date_time_utc)) %>%
                        summarise(n = n())
kable(observations_per_year)
```

```{r warning = FALSE, message = FALSE}
air_data_2$year <- year(air_data_2$date_time_utc)
air_data_2 <- air_data_2 %>% filter(year != '2018')

```

We add to the dataset several more time variables. 

```{r warning = FALSE, message = FALSE}

air_data_2$month <- month(air_data_2$date_time_utc)
air_data_2$year_month_day <- ymd(air_data_2$date_time_utc)
air_data_2$week_day <- wday(air_data_2$date_time_utc, week_start = getOption("lubridate.week.start", 1))
air_data_2$hour <- hour(air_data_2$date_time_utc)
```

We take a look to the general trend of several indicators through the last 18 years

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
# We calcule the yearly mean of the pollutants levels.
year_avgs <- air_data_2 %>% select(station_alias, date_time_utc, PM10, PM25, SO2, NO2, NO, O3, BEN, CO, MXIL, TOL) %>%
  group_by(station_alias, year = year(date_time_utc)) %>%
  summarise_all(funs(mean(., na.rm = TRUE))) %>% 
  select(-date_time_utc) # We drop this variable

# We convert the table to long format

year_avgs_long <- gather(year_avgs, contaminante, value, 3:length(year_avgs)) %>% 
                    filter(!(station_alias == 'Constitución' & year == '2006' & contaminante %in% c('BEN', 'MXIL', 'TOL'))) %>% # We filter this data because is only completed in 0.01%
                    filter(!(station_alias == 'Constitución' & year == '2008' & contaminante == 'PM25')) # We filter this data because is only completed in 0.02%

# We present the data in a grid of graphs

ggplot(year_avgs_long, aes(x = year, y = value)) + 
  geom_line() + 
  facet_grid(contaminante~station_alias,scales="free_y") +
   theme(axis.text = element_text(size = 6))

```


We drop the Santa Bárbara and Montevil stations. These stations have much less data and the behavior of their variables are significantly different (they are sub-urban stations). So, we take them out from the analysis for now. 

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

air_data_3 <- air_data_2 %>% filter(station_alias != 'Montevil' , 
                                         station_alias != 'Santa Bárbara' )

```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
# We calcule the yearly mean of the pollutants levels.
year_avgs <- air_data_3 %>% select(station_alias, date_time_utc, PM10, PM25, SO2, NO2, NO, O3, BEN, CO, MXIL, TOL) %>%
  group_by(station_alias, year = year(date_time_utc)) %>%
  summarise_all(funs(mean(., na.rm = TRUE))) %>% 
  select(-date_time_utc) # quito ahora esta variable, porque no tiene sentido que salga su media.

# We convert the table to long format

year_avgs_long <- gather(year_avgs, contaminante, value, 3:length(year_avgs)) %>%
                    filter(!(station_alias == 'Constitución' & year == '2006' & contaminante %in% c('BEN', 'MXIL', 'TOL'))) %>% # We filter this data because is only completed in 0.01%
                    filter(!(station_alias == 'Constitución' & year == '2008' & contaminante == 'PM25')) # We filter this data because is only completed in 0.02%

# We present the data in a grid of graphs

ggplot(year_avgs_long, aes(x = year, y = value)) + 
  geom_line() + 
  facet_grid(contaminante~station_alias,scales="free_y") +
   theme(axis.text = element_text(size = 6))

```


```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
# We calcule the hourly mean of the pollutants levels.
hour_avgs <- air_data_3 %>% select(station_alias, hour, PM10, PM25, SO2, NO2, NO, O3, BEN, CO, MXIL, TOL) %>%
  group_by(station_alias, hour) %>%
  summarise_all(funs(mean(., na.rm = TRUE)))  # quito ahora esta variable, porque no tiene sentido que salga su media.

# We convert the table to long format

hour_avgs_long <- gather(hour_avgs, contaminante, value, 3:length(hour_avgs))

# We present the data in a grid of graphs

ggplot(hour_avgs_long, aes(x = hour, y = value)) + 
  geom_line() + 
  facet_grid(contaminante~station_alias,scales="free_y") +
   theme(axis.text = element_text(size = 6))
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
# We calcule the monthly mean of the pollutants levels.
month_avgs <- air_data_3 %>% select(station_alias, month, PM10, PM25, SO2, NO2, NO, O3, BEN, CO, MXIL, TOL) %>%
  group_by(station_alias, month) %>%
  summarise_all(funs(mean(., na.rm = TRUE)))  # quito ahora esta variable, porque no tiene sentido que salga su media.

# We convert the table to long format

month_avgs_long <- gather(month_avgs, contaminante, value, 3:length(month_avgs))

# We present the data in a grid of graphs

ggplot(month_avgs_long, aes(x = month, y = value)) + 
  geom_line() + 
  facet_grid(contaminante~station_alias,scales="free_y") +
   theme(axis.text = element_text(size = 6))
```

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}
# We calcule the weekly mean of the pollutants levels.
week_day_avgs <- air_data_3 %>% select(station_alias, week_day, PM10, PM25, SO2, NO2, NO, O3, BEN, CO, MXIL, TOL) %>%
  group_by(station_alias, week_day) %>%
  summarise_all(funs(mean(., na.rm = TRUE)))  # quito ahora esta variable, porque no tiene sentido que salga su media.

# We convert the table to long format

week_day_avgs_long <- gather(week_day_avgs, contaminante, value, 3:length(week_day_avgs))

# We present the data in a grid of graphs

ggplot(week_day_avgs_long, aes(x = week_day, y = value)) + 
  geom_line() + 
  facet_grid(contaminante~station_alias,scales="free_y") +
   theme(axis.text = element_text(size = 6))
```


### Prediction models

We are going to use as base model for our predictions the ARIMA method. And, as first step we are going to try to predict the values of the PM10 pollutant for the Constitución station. 

We create the dataset pm10 with PM10 values from the Constitución Station and we execute a summary


```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

pm10 <- air_data_3 %>% filter(station_alias == 'Constitución') %>%
                        select(date_time_utc, PM10)

summary(pm10)


```

25% of the values are between 44.00 and 888.00. 888.00 is a value really extreme. How many extreme values (outliers) do we have in this series? We plot all the values to visualise this:
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

ggplot(pm10, aes(x = date_time_utc, y = PM10)) + 
         geom_point(alpha = 0.1)

```
We have very few values greater than 250. So, it doesn't seem we have a problem with the outliers (Pending: A PM10 level of 880 is something possible or is it likely to be a monitoring error?).


Daily averages

We create a new dataset with the PM10 daily averages and we plot them in a new graphic. We add a trend line too. There is a clear downward trend in the measurements and we have many fewer extreme values during the last decade. It seems like we have two very clear "epochs" in the data, before and after the year 2008. 

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}


pm10_day_avg <- pm10 %>% group_by(day = date(date_time_utc)) %>%
                          summarise(day_avg = mean(PM10, na.rm = TRUE))

ggplot(pm10_day_avg, aes(x = day, y = day_avg, , colour = day_avg)) + 
         geom_point(alpha = 0.5) +
         geom_smooth(color = "grey", alpha = 0.2) +
         scale_colour_gradientn(colours = terrain.colors(10)) +
         theme(legend.position = c(0.3, 0.9),
                legend.background = element_rect(colour = "transparent", fill = NA), legend.direction = "horizontal") +
         labs(colour = "PM10 daily average (colour scale)", x = "Year", y = "PM10 daily average", title = "PM10 daily average - 2000-2017 evolution (Constitución Station)")

```

We identify a very clear trend through the years on the last graph. But, as we already saw  before on the grid graphs there are other things happening  at the same time. 
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

year_const <- year_avgs_long %>% filter(station_alias == "Constitución", contaminante == 'PM10')
plot1 <- ggplot(year_const, aes(x = year, y = value)) + 
  geom_line()

month_const <- month_avgs_long %>% filter(station_alias == "Constitución", contaminante == 'PM10')
plot2 <- ggplot(month_const, aes(x = month, y = value)) + 
  geom_line()

week_day_const <- week_day_avgs_long %>% filter(station_alias == "Constitución", contaminante == 'PM10')
plot3 <- ggplot(week_day_const, aes(x = week_day, y = value)) + 
  geom_line()

hour_const <- hour_avgs_long %>% filter(station_alias == "Constitución", contaminante == 'PM10')
plot4 <- ggplot(hour_const, aes(x = hour, y = value)) + 
  geom_line()

grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)

```

We create a time series object with the monthly averages. 

```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

year_month_pm10 <- pm10 %>% group_by(year = year(date_time_utc), month = month(date_time_utc)) %>%
                            summarise(year_month_avg = mean(PM10, na.rm = TRUE))

year_month_pm10 <- year_month_pm10 %>% unite("year_month", c("year", "month"), sep = "-")
                
pm10_month_ts <- ts(year_month_pm10$year_month_avg, start = 2000, frequency = 12)            


```
And we plot it.
```{r warning = FALSE, message= FALSE, out.width= '\\textwidth'}

plot(pm10_month_ts)

```

 


#### References

- OpenAir package web: http://davidcarslaw.github.io/openair/
- Breezometer: https://breezometer.com/real-time-air-quality

