[
["index.html", "Gijón Air Pollution 1 Preface", " Gijón Air Pollution Sergio Berdiales 1 Preface My name is Sergio Berdiales and I am a Data Analyst with 10+ years of experience in Customer Experience and Quality areas. If you want to know more about me or contacting me you can visit my Linkedin profile or my Twitter account. This is my final project for the Kschool Master on Data Science (8th edition). The main objective of this project is to show I can apply in a practical way the adquired knowledge during the master. The Master on Data Science of Kschool is a 230 hours course which includes Python and R programming, Statistics, Machile Learning methods, Visualization tools, a Deep Learning introduction and much more (use of Git / Github, linux command line, Jupyter and Google Collab notebooks, etc). And all of these in a very practical and useful way. If you are interested in becoming a Data Scientist this course could be your first step. This is not a scientific paper. And I am not an specialist on air pollution. If you are interested in learning about air pollution my advice is to start visiting any of this web sites: The site of the EU (European Union) on Air Pollution. The site of the EPA (The United States Environmental Protection Agency) on Air Pollution. The site of the WHO (World Health Organization) on Air Pollution. And if your interest is specifically about the situation of the air quality at Gijón city I highly recommend you to read these reports. “Plan de mejora de la calidad del aire en la aglomeración área de Gijón.” (pdf here). “Calidad del Aire y Salud en Asturias. Informe Epidemiológico 2016.” (pdf here). “Informe de calidad del aire en Asturias.”(pdf here). “Estudio de contribución de fuentes en las partículas PM10 en suspensión en la aglomeración área de Gijón y en la zona de Avilés.” (pdf here). “Modelización de la contaminación por partículas PM10 en la aglomeración de Gijón.” (pdf here). This is a work in progress. And my intention is that it is just the start of something much bigger. The next step is to improve the forecasts of pollutants levels including the weather forecasts in the models. So, I think this models would give more accurate predictions. Then, I would put the models in production in order to give predictions on real time via Twitter. I am learning. And there is a huge ammount of things I don’t know or I’m not getting right. For sure. So, if you see something wrong in my code, my reasoning or whatever you think I could improve or fix, please, tell me. I would really appreciate your help. You can contact me via Linkedin profile, Twitter account or Gmail. Structure of this document This document is divided in two basic parts. Project Memory. From the “Preface” to the “References” section would be the memory of the project. The purpose of this section is to explain in a few words what are the objectives of this project, which methodology I used in order to achieve these objectives, and what are my final conclusions. R and Python code. After this first section I included all the R code used (this R code is saved too as rmarkdown files in the Github project repository). The Python code is not included in this document but you have the links to the Google Collab notebooks at the Python scripts section. All the Python notebooks are in the Github repository project too as Jupyter notebooks. "],
["intro.html", "2 Introduction 2.1 Why 2.2 What 2.3 How", " 2 Introduction 2.1 Why Why did I choose this topic, the Air Pollution at the city of Gijón, for my Master of Data Science final project? I was born in Gijón. And I was working 12 years in Madrid. Both cities suffer from air quality low levels. And in recent years the air pollution has become a great source of public concern. So, the topic came up to me in a very natural way. Then, I saw that the city town of Gijón had an open data web. And where it had published 18 years of hourly data from its six air quality monitoring stations. That was definitive. I had a topic. And I had the data. 2.2 What The main objectives of this Data Science project are: 1. Construct and publish a web dashboard to visualize the evolution of the main air pollution indicators of Gijon city. It could seem something trivial and with litte utility. But the fact is that, after searching the web, I didn’t find any service where these kind of evolutions can be consulted easily. This lack of public information can be contributing to generate a very pessimistic vision of the air quality of the city of Gijón. It is true that Gijón presents air quality levels on some pollutants below the recommendations of the WHO. But, the fact is that the Gijón air quality has improved in a drastic way during the last two decades. “include graphic of main pollutants levels evolution” Probably I am being too optimistic, but I would like to think that through this work I’m helping to improve the public understanding on the matter. In order to accomplish this objective I finally choose as main tool the Tableau software and its free service of visualizations web publishing Tableau Public. If you want to see the final result you can go to the chapter 4 “Visualizations”. 2. Build forecasting models to predict pollution levels. My second objective is to try to forecast hourly pollution levels with one to twenty four hours in advance. Initially, my intention was to create models for every pollutant and every monitoring station. But, following the advice of one of the Master professors, I have focused my efforts on only one station (Constitución) and only two of the pollutants, the PM10 and the NO2. I choose the Constitución station because is the unique station which has published weather data too. 2.3 How R for cleaning and exploring the data and for preparing the data for the models. Python, and mostly Sklearn for making the models (except the ARIMA models, made with R and the fantastic Forecast package). Bookdown package to build the html book. And Bookdown.org publishing free service to publish the final document. I used Git and Github for control version of mi work. All the code, data files (except the final table because its size) and other files needed are in this [Github repository]https://github.com/sergioberdiales/TFM_KSchool_Gijon_Air_Pollution I used the Kaggle platform to upload the final dataset with all the data. I used R-Studio on a Windows laptop to manage all the R code and Google Collab notebooks to write the Python code. Tableau Desktop (student license) to create the dashboards and Tableau Public to publish the final stories. "],
["the-data.html", "3 The Data", " 3 The Data “In God we trust. All others must bring data.” – W. Edwards Deming, statistician, professor and author. I gathered the data used in this project from the open data web portal of the town hall of Gijón https://transparencia.gijon.es/. The data can be downloaded from here: Describe the data: kind of data stored. pollutant data, weather data, geographical data. number of monitoring stations. short description of them. I downloaded 18 csv files with air pollution and weather data of Gijón from years 2000 to 2017. I saved them in the “data” folder. I downloaded two more files from this web, a csv file with the description of the variables and another csv file with information about the measurement stations. There are more csv files, with new variables, created ad hoc for the visualizations on my Tableau Public site. They are related and described in the ‘Visualizations’ section of this document. Reference to the train / test / validation datasets created. All the data files are in the Github repository project. Except the air_data_2.csv. I uploaded this file tho Kaggle. Holiday dates csv file. Image source: “Informe de calidad del aire del Principado de Asturias (2016)”. These are the original fields from the 18 csv files downloaded: Estación: Station id. Título: Station name. latitud: Latitude. longitud: Longitude. date_time_utc: Date Time UTC. date_time_utc: Date Time UTC. SO2: SO2 concentration (µg/m³). NO: NO concentration (µg/m³). NO2: NO2 concentration (µg/m³). CO: NO2 concentration (mg/m³). PM10: Particulate Matter (&lt;10 µg/m³). O3: Ozone concentration (µg/m³). dd: Wind direction (degrees). vv: Wind speed (m/s). TMP: Dry temperature (ºC). HR: Relative humidity (%rh). PRB: Atmospheric pressure (mb). RS: Solar radiation (W/m²). LL: Rainfall (l/m²). BEN: Benzene concentration (µg/m³). TOL: Toluene concentration (µg/m³). MXIL: M-Xylene (µg/m³). PM25: Particulate Matter (&lt;2.5 µg/m³). Relate transformations made to the original data. Short summary of the final data table. And these are the fields of the final file ‘air_data_2.csv’: station: Station id. station_name: Station name. latitude: Latitude. longitude: Longitude. date_time_utc: Date Time UTC. SO2: SO2 concentration (µg/m³). NO: NO concentration (µg/m³). NO2: NO2 concentration (µg/m³). CO: NO2 concentration (mg/m³). PM10: Particulate Matter (&lt;10 µg/m³). O3: Ozone concentration (µg/m³). dd: Wind direction (degrees). vv: Wind speed (m/s). TMP: Dry temperature (ºC). HR: Relative humidity (%rh). PRB: Atmospheric pressure (mb). RS: Solar radiation (W/m²). LL: Rainfall (l/m²). BEN: Benzene concentration (µg/m³). TOL: Toluene concentration (µg/m³). MXIL: M-Xylene (µg/m³). PM25: Particulate Matter (&lt;2.5 µg/m³). station_alias: Station alias (new variable). year: Year (new variable). month: Month (new variable). week_day: Week day (new variable). hour: Hour of the day (new variable). date: Date YYYY-MM-DD (new variable). lab: lab = working day / no_lab = no working day. wd: Wind direction in factor format. "],
["visualizations.html", "4 Visualizations", " 4 Visualizations To cover the first of this project goals, “to construct a web dashboard to visualize the evolution of the main air pollution indicators of Gijon city” I chose as main tool the Tableau software. I created a Tableau story for each pollutant with Tableau Desktop (student license). And I published them on my Tableau Public personal site. Each story shows a brief description of each pollutant, the evolution of its levels during the last two decades, the non-compliance with the EU standards or the WHO advice, several visualizations to show its seasonal components, etc. The datasets used on these visualizations were prepared with R code. You can consult this code in the section ***** of this document or in the Github repository of this project *****. Tableau Public stories: If you want to consult the published stories you only have to click on the links below: Pollutant PM10 story Pollutant PM25 story Pollutant NO2 story Pollutant SO2 story Pollutant CO story Pollutant O3 story The data visualization is a basic but powerful tool for any data science project. So, beyond the Tableau dashboards, I used the graphical visualizations of data for very different purposes. From basic data exploration - looking for quality data problems -, to look for data trends and seasonality or to check graphically the accuracy of a model. Some of these visualizations are included in this document but most of them are embedded in the R and Python code. "],
["prediction-models.html", "5 Prediction Models 5.1 ARIMA 5.2 Machine Learning Methods 5.3 Neural Networks", " 5 Prediction Models “Prediction is very difficult, especially if it’s about the future.” Nils Bohr, Nobel laureate in Physics Making predictions with different approaches. 5.1 ARIMA “I have seen the future and it is very much like the present, only longer.” Kehlog Albran, The Profit 5.2 Machine Learning Methods xkcd comic 5.3 Neural Networks "],
["conclusions.html", "6 Conclusions", " 6 Conclusions Conclusions and more. "],
["references.html", "References", " References "],
["r-scripts.html", "R scripts", " R scripts "],
["gathering-and-cleaning-data.html", "7 Gathering and Cleaning Data 7.1 Data gathering 7.2 Data cleaning 7.3 Adding new variables", " 7 Gathering and Cleaning Data 7.1 Data gathering Loading packages library(readr) library(dplyr) library(tidyr) library(purrr) library(lubridate) library(ggplot2) library(stringr) library(knitr) library(xts) library(zoo) library(gridExtra) library(fpp2) library(RcppRoll) library(kableExtra) options(knitr.table.format = &quot;html&quot;) First of all I have to check if I will have the basic data to make the analysis. I need air pollution and weather data of the Gijon area. The town hall of Gijon has an open data web portal here https://transparencia.gijon.es/. We can download pollution air data on csv format from year 2000 to 2017 here: I downloaded 18 csv files with air pollution and weather data of Gijón from years 2000 to 2017. I saved them in the “data” folder. I downloaded two more files from this web, a csv file with the description of the variables and another csv file with information about the measurement stations. We take a look to the information included in the stations_info.csv file. It includes the stations addresses, longitude, latitude and their IDs and names. All this information, as we will see, is included in the csv files with pollution and weather data too. So, we are not going to use this file anymore. stations &lt;- read_delim(&#39;data/stations_info.csv&#39;, delim = &#39;;&#39;, escape_double = FALSE, trim_ws = TRUE, locale = locale(encoding = &quot;ISO-8859-1&quot;), col_types = cols(.default = &quot;c&quot;)) stations ## # A tibble: 6 x 6 ## `&quot;ID;&quot;&quot;Título&quot;&quot;` `&quot;&quot;Dirección&quot;&quot;` `&quot;&quot;Población&quot;&quot;` `&quot;&quot;Provincia&quot;&quot;` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;\\&quot;1;\\&quot;\\&quot;Consti~ &quot;\\&quot;\\&quot;Avda. Con~ &quot;\\&quot;\\&quot;Gijón\\&quot;\\&quot;&quot; &quot;\\&quot;\\&quot;Asturias\\~ ## 2 &quot;\\&quot;2;\\&quot;\\&quot;Argent~ &quot;\\&quot;\\&quot;Avda. Arg~ &quot;\\&quot;\\&quot;Gijón\\&quot;\\&quot;&quot; &quot;\\&quot;\\&quot;Asturias\\~ ## 3 &quot;\\&quot;3;\\&quot;\\&quot;H. Fel~ &quot;\\&quot;\\&quot;H. Felgue~ &quot;\\&quot;\\&quot;Gijón\\&quot;\\&quot;&quot; &quot;\\&quot;\\&quot;Asturias\\~ ## 4 &quot;\\&quot;4;\\&quot;\\&quot;Castil~ &quot;\\&quot;\\&quot;Plaza Cas~ &quot;\\&quot;\\&quot;Gijón\\&quot;\\&quot;&quot; &quot;\\&quot;\\&quot;Asturias\\~ ## 5 &quot;\\&quot;10;\\&quot;\\&quot;Monte~ &quot;\\&quot;\\&quot;Montevil\\~ &quot;\\&quot;\\&quot;Gijón\\&quot;\\&quot;&quot; &quot;\\&quot;\\&quot;Asturias\\~ ## 6 &quot;\\&quot;11;\\&quot;\\&quot;Santa~ &quot;\\&quot;\\&quot;Santa Bár~ &quot;\\&quot;\\&quot;Gijón\\&quot;\\&quot;&quot; &quot;\\&quot;\\&quot;Asturias\\~ ## # ... with 2 more variables: `&quot;&quot;latitud&quot;&quot;` &lt;chr&gt;, `&quot;&quot;longitud&quot;&quot;&quot;,,` &lt;chr&gt; We can see on this image the location of each station. http://movil.asturias.es/medioambiente/articulos/ficheros/Informe%20de%20calidad%20del%20aire%20en%20Asturias%202016.pdf Image source: “Informe de calidad del aire del Principado de Asturias (2016)”. The air_data_descriptors.csv file contains information about the nature of the elements monitored by the stations. Names, descriptions and units. variables &lt;- read_csv(&#39;data/air_data_descriptors.csv&#39;, locale = locale(encoding = &quot;ISO-8859-1&quot;)) variables ## # A tibble: 17 x 4 ## Parametro `Descripción Parámetro` TAG Unidad ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 BEN Benceno BEN µg/m³ ## 2 CO Concentracion de CO CO mg/m³ ## 3 DD Direccion del viento DD Grados ## 4 HR Humedad relativa HR %hr ## 5 LL Precipitacion LL l/m² ## 6 MXIL MXileno MXIL µg/m³ ## 7 NO Concentracion de NO NO µg/m³ ## 8 NO2 Concentracion de NO2 NO2 µg/m³ ## 9 O3 Concentracion de Ozono O3 µg/m³ ## 10 PM10 Particulas en suspension &lt;10 µg/m³ PM10 µg/m³ ## 11 PM25 Particulas en Suspension PM 2,5 PM25 µg/m³ ## 12 PRB Presion Atmosferica PRB mb ## 13 RS Radiacion Solar RS W/m² ## 14 SO2 Concentracion de SO2 SO2 µg/m³ ## 15 TMP Temperatura Seca TMP ºC ## 16 TOL Tolueno TOL µg/m³ ## 17 VV Velocidad del viento VV m/s In order to import the data from the 18 csv files we list all the files in the object data_files. data_files &lt;- list.files(path = &quot;data&quot;, pattern = &quot;air_data_20*&quot;) Then, we map the function read_csv on this list in order to import every file and finally merge them in a unique dataframe (air_data_0) with reduce(rbind). air_data_0 &lt;- data_files %&gt;% map(function(x) { read_csv(paste0(&quot;./data/&quot;, x), locale = locale(encoding = &quot;ISO-8859-1&quot;), col_types = cols(.default = &quot;c&quot;)) }) %&gt;% reduce(rbind) We take a look to the dataset glimpse(air_data_0) ## Observations: 722,774 ## Variables: 22 ## $ Estación &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1... ## $ Título &lt;chr&gt; &quot;Estación Avenida Constitución&quot;, &quot;Estación... ## $ latitud &lt;chr&gt; &quot;43.529806&quot;, &quot;43.529806&quot;, &quot;43.529806&quot;, &quot;43... ## $ longitud &lt;chr&gt; &quot;-5.673428&quot;, &quot;-5.673428&quot;, &quot;-5.673428&quot;, &quot;-5... ## $ `Fecha Solar (UTC)` &lt;chr&gt; &quot;2000-01-01T00:00:00&quot;, &quot;2000-01-01T01:00:0... ## $ SO2 &lt;chr&gt; &quot;23&quot;, &quot;29&quot;, &quot;40&quot;, &quot;50&quot;, &quot;39&quot;, &quot;39&quot;, &quot;40&quot;, ... ## $ NO &lt;chr&gt; &quot;89&quot;, &quot;73&quot;, &quot;53&quot;, &quot;46&quot;, &quot;35&quot;, &quot;26&quot;, &quot;27&quot;, ... ## $ NO2 &lt;chr&gt; &quot;65&quot;, &quot;60&quot;, &quot;57&quot;, &quot;53&quot;, &quot;50&quot;, &quot;49&quot;, &quot;51&quot;, ... ## $ CO &lt;chr&gt; &quot;1.97&quot;, &quot;1.61&quot;, &quot;1.13&quot;, &quot;1.06&quot;, &quot;0.95&quot;, &quot;0... ## $ PM10 &lt;chr&gt; &quot;53&quot;, &quot;63&quot;, &quot;56&quot;, &quot;58&quot;, &quot;50&quot;, &quot;50&quot;, &quot;57&quot;, ... ## $ O3 &lt;chr&gt; &quot;9&quot;, &quot;8&quot;, &quot;7&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;7&quot;, &quot;4&quot;, &quot;5... ## $ dd &lt;chr&gt; &quot;245&quot;, &quot;222&quot;, &quot;228&quot;, &quot;239&quot;, &quot;244&quot;, &quot;218&quot;, ... ## $ vv &lt;chr&gt; &quot;0.34&quot;, &quot;1.06&quot;, &quot;0.71&quot;, &quot;0.84&quot;, &quot;0.89&quot;, &quot;0... ## $ TMP &lt;chr&gt; &quot;5.7&quot;, &quot;5.4&quot;, &quot;5.3&quot;, &quot;5.1&quot;, &quot;4.6&quot;, &quot;4.6&quot;, ... ## $ HR &lt;chr&gt; &quot;76&quot;, &quot;73&quot;, &quot;72&quot;, &quot;71&quot;, &quot;72&quot;, &quot;69&quot;, &quot;68&quot;, ... ## $ PRB &lt;chr&gt; &quot;1026&quot;, &quot;1025&quot;, &quot;1025&quot;, &quot;1025&quot;, &quot;1024&quot;, &quot;1... ## $ RS &lt;chr&gt; &quot;33&quot;, &quot;33&quot;, &quot;33&quot;, &quot;33&quot;, &quot;33&quot;, &quot;33&quot;, &quot;33&quot;, ... ## $ LL &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0... ## $ BEN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA... ## $ TOL &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA... ## $ MXIL &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA... ## $ PM25 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA... We change some variables names. # Variables names changing air_data_1 &lt;- air_data_0 %&gt;% rename(station = `Estación`, station_name = `Título`, date_time_utc = `Fecha Solar (UTC)`, latitude = latitud, longitude = longitud) 7.2 Data cleaning We imported all the columns as characters in order to avoid problems with the format attributions. So, we have to make now some format variable changes. We change the date_time_utc format from character to date time. air_data_1$date_time_utc &lt;- ymd_hms(air_data_1$date_time_utc) We change the station and station_name formats from character to factor. air_data_1$station &lt;- as.factor(air_data_1$station) air_data_1$station_name &lt;- as.factor(air_data_1$station_name) We create a vector with all the variables we want to be numeric num &lt;- colnames(air_data_1)[c(3, 4, 6:22)] We make the conversion of this set of variables to numeric air_data_1 &lt;- air_data_1 %&gt;% mutate_at(num, as.numeric) We create a dictionary with an alias for each station in order to add a new variable with more convenient station names alias_dict &lt;- data.frame( station = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;10&quot;, &quot;11&quot;), station_alias = c(&quot;Constitución&quot;, &quot;Argentina&quot;, &quot;H. Felgueroso&quot;, &quot;Castilla&quot;, &quot;Montevil&quot;, &quot;Santa Bárbara&quot;) ) We join the alias dictionary to the air_data_1 data frame to add the new variable to the data set. air_data_1 &lt;- air_data_1 %&gt;% left_join(alias_dict, by = &#39;station&#39;) We call the summary function to inspect the data main indicators summary(air_data_1) ## station station_name latitude ## 1 :157727 Estación Avenida Argentina :157798 Min. :43.52 ## 10: 74630 Estación Avenida Castilla :157409 1st Qu.:43.53 ## 11: 17544 Estación Avenida Constitución :157727 Median :43.54 ## 2 :157798 Estación Avenida Hermanos Felgueroso:157666 Mean :43.53 ## 3 :157666 Estación de Montevil : 74630 3rd Qu.:43.54 ## 4 :157409 Estación Santa Bárbara : 17544 Max. :43.54 ## ## longitude date_time_utc SO2 ## Min. :-5.699 Min. :2000-01-01 00:00:00 Min. :-9999.00 ## 1st Qu.:-5.673 1st Qu.:2005-02-25 05:00:00 1st Qu.: 4.00 ## Median :-5.672 Median :2010-02-23 11:00:00 Median : 6.00 ## Mean :-5.670 Mean :2009-09-06 07:33:13 Mean : 9.77 ## 3rd Qu.:-5.658 3rd Qu.:2014-04-09 06:00:00 3rd Qu.: 11.00 ## Max. :-5.646 Max. :2018-01-01 00:00:00 Max. : 2662.00 ## NA&#39;s :33742 ## NO NO2 CO PM10 ## Min. :-9999.00 Min. :-9999.00 Min. : 0.00 Min. :-9999.00 ## 1st Qu.: 4.40 1st Qu.: 16.00 1st Qu.: 0.22 1st Qu.: 19.00 ## Median : 10.00 Median : 28.00 Median : 0.36 Median : 30.00 ## Mean : 21.37 Mean : 32.04 Mean : 0.49 Mean : 35.88 ## 3rd Qu.: 23.00 3rd Qu.: 45.00 3rd Qu.: 0.59 3rd Qu.: 46.00 ## Max. : 1248.00 Max. : 1003.20 Max. :58.20 Max. : 1000.00 ## NA&#39;s :16989 NA&#39;s :16446 NA&#39;s :90390 NA&#39;s :88598 ## O3 dd vv TMP ## Min. :-9999.00 Min. : 0.0 Min. : 0.0 Min. :-40.0 ## 1st Qu.: 17.00 1st Qu.: 96.0 1st Qu.: 0.2 1st Qu.: 10.9 ## Median : 37.00 Median :159.0 Median : 0.7 Median : 14.7 ## Mean : 38.97 Mean :161.8 Mean : 1.0 Mean : 14.6 ## 3rd Qu.: 57.00 3rd Qu.:228.0 3rd Qu.: 1.5 3rd Qu.: 18.4 ## Max. : 998.00 Max. :360.0 Max. :29.8 Max. : 47.4 ## NA&#39;s :31417 NA&#39;s :494134 NA&#39;s :493893 NA&#39;s :494151 ## HR PRB RS LL ## Min. : 0.0 Min. : 800 Min. : -1.0 Min. : 0.0 ## 1st Qu.: 69.0 1st Qu.:1007 1st Qu.: 17.0 1st Qu.: 0.0 ## Median : 80.0 Median :1013 Median : 46.0 Median : 0.0 ## Mean : 78.3 Mean :1012 Mean : 125.2 Mean : 0.1 ## 3rd Qu.: 89.0 3rd Qu.:1018 3rd Qu.: 149.0 3rd Qu.: 0.0 ## Max. :123.0 Max. :1282 Max. :1470.0 Max. :24.6 ## NA&#39;s :494176 NA&#39;s :494019 NA&#39;s :494273 NA&#39;s :494124 ## BEN TOL MXIL PM25 ## Min. : 0.0 Min. : -0.2 Min. : -0.3 Min. : 0.0 ## 1st Qu.: 0.1 1st Qu.: 0.4 1st Qu.: 0.2 1st Qu.: 5.0 ## Median : 0.3 Median : 1.0 Median : 0.3 Median : 9.0 ## Mean : 0.5 Mean : 2.5 Mean : 1.3 Mean : 11.3 ## 3rd Qu.: 0.5 3rd Qu.: 2.5 3rd Qu.: 0.9 3rd Qu.: 15.0 ## Max. :22.5 Max. :196.0 Max. :220.0 Max. :947.0 ## NA&#39;s :629358 NA&#39;s :629380 NA&#39;s :635123 NA&#39;s :554185 ## station_alias ## Argentina :157798 ## Castilla :157409 ## Constitución :157727 ## H. Felgueroso:157666 ## Montevil : 74630 ## Santa Bárbara: 17544 ## There are several variables which minimun values are -9999. kable(air_data_1 %&gt;% filter(SO2 == -9999 | NO == -9999 | NO2 == -9999 | PM10 == -9999 | O3 == -9999 )) %&gt;% kable_styling() station station_name latitude longitude date_time_utc SO2 NO NO2 CO PM10 O3 dd vv TMP HR PRB RS LL BEN TOL MXIL PM25 station_alias 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 00:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 01:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 02:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 03:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 04:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 05:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 06:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 07:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 08:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 09:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 10:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 11:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 12:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 13:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 14:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 15:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 16:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 17:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 18:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 19:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 20:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 21:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 22:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso 3 Estación Avenida Hermanos Felgueroso 43.53506 -5.658123 2000-01-27 23:00:00 -9999 -9999 -9999 0 -9999 -9999 NA NA NA NA NA NA NA NA NA NA NA H. Felgueroso They are all from the same day (2000-01-27) and from the same station (‘H. Felgueroso’). We replace these values by NAs. air_data_2 &lt;- air_data_1 %&gt;% mutate(SO2 = replace(SO2, SO2 == -9999, NA), NO = replace(NO, NO == -9999, NA), NO2 = replace(NO2, NO2 == -9999, NA), PM10 = replace(PM10, PM10 == -9999, NA), O3 = replace(O3, O3 == -9999, NA)) We check again the output of the summary function. summary(air_data_2) ## station station_name latitude ## 1 :157727 Estación Avenida Argentina :157798 Min. :43.52 ## 10: 74630 Estación Avenida Castilla :157409 1st Qu.:43.53 ## 11: 17544 Estación Avenida Constitución :157727 Median :43.54 ## 2 :157798 Estación Avenida Hermanos Felgueroso:157666 Mean :43.53 ## 3 :157666 Estación de Montevil : 74630 3rd Qu.:43.54 ## 4 :157409 Estación Santa Bárbara : 17544 Max. :43.54 ## ## longitude date_time_utc SO2 ## Min. :-5.699 Min. :2000-01-01 00:00:00 Min. : -2.00 ## 1st Qu.:-5.673 1st Qu.:2005-02-25 05:00:00 1st Qu.: 4.00 ## Median :-5.672 Median :2010-02-23 11:00:00 Median : 6.00 ## Mean :-5.670 Mean :2009-09-06 07:33:13 Mean : 10.12 ## 3rd Qu.:-5.658 3rd Qu.:2014-04-09 06:00:00 3rd Qu.: 11.00 ## Max. :-5.646 Max. :2018-01-01 00:00:00 Max. :2662.00 ## NA&#39;s :33766 ## NO NO2 CO PM10 ## Min. : 0.00 Min. : 0.00 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 4.40 1st Qu.: 16.00 1st Qu.: 0.22 1st Qu.: 19.00 ## Median : 10.00 Median : 28.00 Median : 0.36 Median : 30.00 ## Mean : 21.71 Mean : 32.38 Mean : 0.49 Mean : 36.26 ## 3rd Qu.: 23.00 3rd Qu.: 45.00 3rd Qu.: 0.59 3rd Qu.: 46.00 ## Max. :1248.00 Max. :1003.20 Max. :58.20 Max. :1000.00 ## NA&#39;s :17013 NA&#39;s :16470 NA&#39;s :90390 NA&#39;s :88622 ## O3 dd vv TMP ## Min. : 0.00 Min. : 0.0 Min. : 0.0 Min. :-40.0 ## 1st Qu.: 17.00 1st Qu.: 96.0 1st Qu.: 0.2 1st Qu.: 10.9 ## Median : 37.00 Median :159.0 Median : 0.7 Median : 14.7 ## Mean : 39.32 Mean :161.8 Mean : 1.0 Mean : 14.6 ## 3rd Qu.: 57.00 3rd Qu.:228.0 3rd Qu.: 1.5 3rd Qu.: 18.4 ## Max. :998.00 Max. :360.0 Max. :29.8 Max. : 47.4 ## NA&#39;s :31441 NA&#39;s :494134 NA&#39;s :493893 NA&#39;s :494151 ## HR PRB RS LL ## Min. : 0.0 Min. : 800 Min. : -1.0 Min. : 0.0 ## 1st Qu.: 69.0 1st Qu.:1007 1st Qu.: 17.0 1st Qu.: 0.0 ## Median : 80.0 Median :1013 Median : 46.0 Median : 0.0 ## Mean : 78.3 Mean :1012 Mean : 125.2 Mean : 0.1 ## 3rd Qu.: 89.0 3rd Qu.:1018 3rd Qu.: 149.0 3rd Qu.: 0.0 ## Max. :123.0 Max. :1282 Max. :1470.0 Max. :24.6 ## NA&#39;s :494176 NA&#39;s :494019 NA&#39;s :494273 NA&#39;s :494124 ## BEN TOL MXIL PM25 ## Min. : 0.0 Min. : -0.2 Min. : -0.3 Min. : 0.0 ## 1st Qu.: 0.1 1st Qu.: 0.4 1st Qu.: 0.2 1st Qu.: 5.0 ## Median : 0.3 Median : 1.0 Median : 0.3 Median : 9.0 ## Mean : 0.5 Mean : 2.5 Mean : 1.3 Mean : 11.3 ## 3rd Qu.: 0.5 3rd Qu.: 2.5 3rd Qu.: 0.9 3rd Qu.: 15.0 ## Max. :22.5 Max. :196.0 Max. :220.0 Max. :947.0 ## NA&#39;s :629358 NA&#39;s :629380 NA&#39;s :635123 NA&#39;s :554185 ## station_alias ## Argentina :157798 ## Castilla :157409 ## Constitución :157727 ## H. Felgueroso:157666 ## Montevil : 74630 ## Santa Bárbara: 17544 ## Some pollutant variables have as minimum negative values. It does not make much sense. We take a look to the data in order to quantify the problem. 30 SO2 observations between 2015-12-25 and 2015-12-28 from the Montevil station: (neg_SO2 &lt;- air_data_2 %&gt;% filter(SO2 &lt; 0) %&gt;% summarise(n = n())) ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 30 2 RS observations from the Constitucion station: (neg_RS &lt;- air_data_2 %&gt;% filter(RS &lt; 0) %&gt;% summarise(n = n())) ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 2 27 TOL observations between the 2008-12-11 and the 2008-12-15 from the Constitucion station: (neg_TOL &lt;- air_data_2 %&gt;% filter(TOL &lt; 0) %&gt;% summarise(n = n())) ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 27 59 MXIL observations between the 2008-12-10 and the 2008-12-15 from the Constitucion station: (neg_MXIL &lt;- air_data_2 %&gt;% filter(MXIL &lt; 0) %&gt;% summarise(n = n())) ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 59 There are not many cases. We replace them all by NAs and call again the summary function. air_data_2 &lt;- air_data_2 %&gt;% mutate(SO2 = replace(SO2, SO2 &lt; 0, NA), RS = replace(RS, RS &lt; 0, NA), TOL = replace(TOL, TOL &lt; 0, NA), MXIL = replace(MXIL, MXIL &lt; 0, NA)) summary(air_data_2) ## station station_name latitude ## 1 :157727 Estación Avenida Argentina :157798 Min. :43.52 ## 10: 74630 Estación Avenida Castilla :157409 1st Qu.:43.53 ## 11: 17544 Estación Avenida Constitución :157727 Median :43.54 ## 2 :157798 Estación Avenida Hermanos Felgueroso:157666 Mean :43.53 ## 3 :157666 Estación de Montevil : 74630 3rd Qu.:43.54 ## 4 :157409 Estación Santa Bárbara : 17544 Max. :43.54 ## ## longitude date_time_utc SO2 ## Min. :-5.699 Min. :2000-01-01 00:00:00 Min. : 0.00 ## 1st Qu.:-5.673 1st Qu.:2005-02-25 05:00:00 1st Qu.: 4.00 ## Median :-5.672 Median :2010-02-23 11:00:00 Median : 6.00 ## Mean :-5.670 Mean :2009-09-06 07:33:13 Mean : 10.12 ## 3rd Qu.:-5.658 3rd Qu.:2014-04-09 06:00:00 3rd Qu.: 11.00 ## Max. :-5.646 Max. :2018-01-01 00:00:00 Max. :2662.00 ## NA&#39;s :33796 ## NO NO2 CO PM10 ## Min. : 0.00 Min. : 0.00 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 4.40 1st Qu.: 16.00 1st Qu.: 0.22 1st Qu.: 19.00 ## Median : 10.00 Median : 28.00 Median : 0.36 Median : 30.00 ## Mean : 21.71 Mean : 32.38 Mean : 0.49 Mean : 36.26 ## 3rd Qu.: 23.00 3rd Qu.: 45.00 3rd Qu.: 0.59 3rd Qu.: 46.00 ## Max. :1248.00 Max. :1003.20 Max. :58.20 Max. :1000.00 ## NA&#39;s :17013 NA&#39;s :16470 NA&#39;s :90390 NA&#39;s :88622 ## O3 dd vv TMP ## Min. : 0.00 Min. : 0.0 Min. : 0.0 Min. :-40.0 ## 1st Qu.: 17.00 1st Qu.: 96.0 1st Qu.: 0.2 1st Qu.: 10.9 ## Median : 37.00 Median :159.0 Median : 0.7 Median : 14.7 ## Mean : 39.32 Mean :161.8 Mean : 1.0 Mean : 14.6 ## 3rd Qu.: 57.00 3rd Qu.:228.0 3rd Qu.: 1.5 3rd Qu.: 18.4 ## Max. :998.00 Max. :360.0 Max. :29.8 Max. : 47.4 ## NA&#39;s :31441 NA&#39;s :494134 NA&#39;s :493893 NA&#39;s :494151 ## HR PRB RS LL ## Min. : 0.0 Min. : 800 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 69.0 1st Qu.:1007 1st Qu.: 17.0 1st Qu.: 0.0 ## Median : 80.0 Median :1013 Median : 46.0 Median : 0.0 ## Mean : 78.3 Mean :1012 Mean : 125.2 Mean : 0.1 ## 3rd Qu.: 89.0 3rd Qu.:1018 3rd Qu.: 149.0 3rd Qu.: 0.0 ## Max. :123.0 Max. :1282 Max. :1470.0 Max. :24.6 ## NA&#39;s :494176 NA&#39;s :494019 NA&#39;s :494275 NA&#39;s :494124 ## BEN TOL MXIL PM25 ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 0.1 1st Qu.: 0.4 1st Qu.: 0.2 1st Qu.: 5.0 ## Median : 0.3 Median : 1.0 Median : 0.3 Median : 9.0 ## Mean : 0.5 Mean : 2.5 Mean : 1.3 Mean : 11.3 ## 3rd Qu.: 0.5 3rd Qu.: 2.5 3rd Qu.: 0.9 3rd Qu.: 15.0 ## Max. :22.5 Max. :196.0 Max. :220.0 Max. :947.0 ## NA&#39;s :629358 NA&#39;s :629407 NA&#39;s :635182 NA&#39;s :554185 ## station_alias ## Argentina :157798 ## Castilla :157409 ## Constitución :157727 ## H. Felgueroso:157666 ## Montevil : 74630 ## Santa Bárbara: 17544 ## We take a look to the data completeness. What proportion of nas do we have by variable, station, year, etc? data_completeness &lt;- air_data_2 %&gt;% group_by(station_alias, year = year(date_time_utc)) %&gt;% summarise_all(funs(round(sum(!is.na(.))/n(), 2))) %&gt;% # We obtain the proportion of &#39;not NAs&#39; select(-c(3:7, 25:28)) # These columns do not have any na. We exclude them. head(data_completeness, 10) %&gt;% kable() %&gt;% kable_styling() station_alias year SO2 NO NO2 CO PM10 O3 dd vv TMP HR PRB RS LL BEN TOL MXIL PM25 Argentina 2000 0.99 0.97 0.97 0.96 0.94 0.97 0 0 0 0 0 0 0 0 0 0 0 Argentina 2001 0.99 0.99 0.99 0.98 0.97 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2002 1.00 0.99 0.99 0.99 0.99 1.00 0 0 0 0 0 0 0 0 0 0 0 Argentina 2003 0.99 0.98 0.98 0.98 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2004 0.98 0.96 0.97 0.99 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Argentina 2005 0.98 0.96 0.98 1.00 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Argentina 2006 0.92 0.90 0.92 0.92 0.93 0.93 0 0 0 0 0 0 0 0 0 0 0 Argentina 2007 0.98 0.99 0.99 0.98 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2008 0.98 0.96 0.98 0.97 0.98 0.98 0 0 0 0 0 0 0 0 0 0 0 Argentina 2009 1.00 1.00 1.00 0.98 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 We are going to check the data completeness by station: Constitución: There is data registered from the variables SO2, NO, NO2, CO, PM10, 03, dd, vv, TMP, HR, PRB, HS and LL since the year 2000. There are measurements of the variables BEN, TOL and MXIL since the year 2006 (only 0.01% ). The PM25 particles are monitored since the year 2008 (2008: only covered 0,02% of the year). During the year 2008 the completeness of several variables (HR, PRB, HS, LL, BEN, TOL y MXIL) decrease until 88% (to do: check there was not caused by a data importing problem.) constitucion_data &lt;- data_completeness %&gt;% filter(station_alias == &#39;Constitución&#39;) constitucion_data %&gt;% kable() %&gt;% kable_styling() station_alias year SO2 NO NO2 CO PM10 O3 dd vv TMP HR PRB RS LL BEN TOL MXIL PM25 Constitución 2000 0.97 0.95 0.95 0.97 0.92 0.93 0.96 0.98 0.96 0.95 0.97 0.95 0.96 0.00 0.00 0.00 0.00 Constitución 2001 0.99 0.99 0.99 0.98 0.99 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.00 0.00 0.00 0.00 Constitución 2002 1.00 1.00 1.00 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 Constitución 2003 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.00 0.00 0.00 0.00 Constitución 2004 0.99 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 Constitución 2005 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.00 0.00 0.00 0.00 Constitución 2006 0.91 0.91 0.91 0.90 0.91 0.91 0.91 0.91 0.91 0.91 0.91 0.91 0.91 0.01 0.01 0.01 0.00 Constitución 2007 0.98 0.99 0.99 0.97 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.00 Constitución 2008 0.98 0.99 0.99 0.99 0.99 1.00 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.02 Constitución 2009 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Constitución 2010 0.99 0.99 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 0.99 Constitución 2011 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.98 0.98 0.99 Constitución 2012 0.97 0.97 0.97 0.96 0.97 0.96 0.97 0.97 0.97 0.97 0.97 0.97 0.97 0.96 0.96 0.96 0.97 Constitución 2013 0.99 0.99 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 Constitución 2014 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 Constitución 2015 0.98 0.98 0.98 0.98 0.99 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.98 0.32 0.98 Constitución 2016 0.95 0.95 0.95 0.95 0.95 0.95 0.98 0.98 0.97 0.97 0.97 0.97 0.97 0.90 0.90 0.90 0.95 Constitución 2017 0.99 0.99 0.99 0.99 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 1.00 Constitución 2018 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Argentina: data since the year 2000. Variables: SO2, NO, NO2, CO, PM10 and 03. argentina_data &lt;- data_completeness %&gt;% filter(station_alias == &#39;Argentina&#39;) argentina_data %&gt;% kable() %&gt;% kable_styling() station_alias year SO2 NO NO2 CO PM10 O3 dd vv TMP HR PRB RS LL BEN TOL MXIL PM25 Argentina 2000 0.99 0.97 0.97 0.96 0.94 0.97 0 0 0 0 0 0 0 0 0 0 0 Argentina 2001 0.99 0.99 0.99 0.98 0.97 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2002 1.00 0.99 0.99 0.99 0.99 1.00 0 0 0 0 0 0 0 0 0 0 0 Argentina 2003 0.99 0.98 0.98 0.98 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2004 0.98 0.96 0.97 0.99 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Argentina 2005 0.98 0.96 0.98 1.00 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Argentina 2006 0.92 0.90 0.92 0.92 0.93 0.93 0 0 0 0 0 0 0 0 0 0 0 Argentina 2007 0.98 0.99 0.99 0.98 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2008 0.98 0.96 0.98 0.97 0.98 0.98 0 0 0 0 0 0 0 0 0 0 0 Argentina 2009 1.00 1.00 1.00 0.98 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Argentina 2010 0.99 0.99 1.00 0.99 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Argentina 2011 0.98 0.99 0.99 0.98 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2012 0.99 0.96 0.96 0.96 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Argentina 2013 0.99 0.99 0.99 0.99 1.00 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2014 1.00 0.99 0.99 1.00 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Argentina 2015 0.99 0.99 0.99 0.99 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2016 0.99 0.99 0.99 0.99 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2017 0.99 0.99 0.99 0.99 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Argentina 2018 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso: data since the year 2000. Variables: SO2, NO, NO2, CO, PM10 and 03. During the year 2006 the completeness of the data decrease until 88% (to do: check there was not caused by a data importing problem.) felgueroso_data &lt;- data_completeness %&gt;% filter(station_alias == &#39;H. Felgueroso&#39;) felgueroso_data %&gt;% kable() %&gt;% kable_styling() station_alias year SO2 NO NO2 CO PM10 O3 dd vv TMP HR PRB RS LL BEN TOL MXIL PM25 H. Felgueroso 2000 0.97 0.96 0.96 0.97 0.96 0.96 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2001 0.99 0.99 0.99 0.99 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2002 0.93 0.93 0.93 0.93 0.93 0.93 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2003 0.98 0.98 0.98 0.97 0.98 0.98 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2004 0.98 0.97 0.97 0.99 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2005 0.97 0.96 0.96 0.99 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2006 0.88 0.87 0.87 0.90 0.90 0.90 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2007 0.98 0.99 0.99 0.99 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2008 0.98 0.99 0.99 0.99 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2009 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2010 0.99 0.99 0.99 0.99 0.98 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2011 0.99 0.99 0.99 1.00 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2012 0.96 0.97 0.97 0.97 0.97 0.97 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2013 0.99 0.99 0.99 0.99 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2014 0.98 0.98 0.98 0.99 0.99 0.98 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2015 1.00 1.00 1.00 1.00 1.00 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2016 0.99 0.99 0.99 0.99 0.98 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2017 0.99 0.99 0.99 0.99 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 H. Felgueroso 2018 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Castilla: data since the year 2000. Variables: SO2, NO, NO2, CO, PM10 and 03. During the year 2015 the completeness of the data decrease until 77% (to do: check there was not caused by a data importing problem.) castilla_data &lt;- data_completeness %&gt;% filter(station_alias == &#39;Castilla&#39;) castilla_data %&gt;% kable() %&gt;% kable_styling() station_alias year SO2 NO NO2 CO PM10 O3 dd vv TMP HR PRB RS LL BEN TOL MXIL PM25 Castilla 2000 0.97 0.97 0.97 0.97 0.97 0.95 0 0 0 0 0 0 0 0 0 0 0 Castilla 2001 0.98 0.99 0.99 0.98 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Castilla 2002 0.99 0.99 0.99 0.97 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Castilla 2003 0.99 0.99 0.99 0.98 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Castilla 2004 0.99 0.99 0.99 0.98 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Castilla 2005 0.99 0.95 0.95 0.98 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Castilla 2006 0.91 0.91 0.91 0.91 0.92 0.93 0 0 0 0 0 0 0 0 0 0 0 Castilla 2007 0.99 1.00 1.00 0.99 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Castilla 2008 0.95 0.96 0.96 0.95 0.96 0.96 0 0 0 0 0 0 0 0 0 0 0 Castilla 2009 0.99 0.99 0.99 0.99 0.99 1.00 0 0 0 0 0 0 0 0 0 0 0 Castilla 2010 0.92 0.93 0.93 0.93 0.93 0.93 0 0 0 0 0 0 0 0 0 0 0 Castilla 2011 0.97 0.99 0.99 0.98 0.99 0.99 0 0 0 0 0 0 0 0 0 0 0 Castilla 2012 0.97 0.98 0.98 0.98 0.98 0.98 0 0 0 0 0 0 0 0 0 0 0 Castilla 2013 1.00 0.99 0.99 1.00 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Castilla 2014 0.99 0.99 0.99 0.99 1.00 0.99 0 0 0 0 0 0 0 0 0 0 0 Castilla 2015 0.77 0.76 0.76 0.77 0.76 0.77 0 0 0 0 0 0 0 0 0 0 0 Castilla 2016 0.98 0.99 0.99 0.99 0.97 0.98 0 0 0 0 0 0 0 0 0 0 0 Castilla 2017 0.97 0.99 0.99 0.99 0.98 0.97 0 0 0 0 0 0 0 0 0 0 0 Castilla 2018 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 Montevil: Data since the year 2009. Variables: SO2, NO, NO2, 03, dd, vv, TMP, HR, PRB, HS, LL and PM25. montevil_data &lt;- data_completeness %&gt;% filter(station_alias == &#39;Montevil&#39;) montevil_data %&gt;% kable() %&gt;% kable_styling() station_alias year SO2 NO NO2 CO PM10 O3 dd vv TMP HR PRB RS LL BEN TOL MXIL PM25 Montevil 2009 0.91 0.93 0.93 0 0 0.93 0.93 0.93 0.93 0.93 0.93 0.93 0.93 0 0 0 0.93 Montevil 2010 0.99 1.00 1.00 0 0 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 0.92 Montevil 2011 0.99 0.99 0.99 0 0 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 1.00 Montevil 2012 1.00 1.00 1.00 0 0 1.00 0.98 0.98 1.00 1.00 1.00 1.00 1.00 0 0 0 1.00 Montevil 2013 1.00 1.00 1.00 0 0 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 1.00 Montevil 2014 1.00 1.00 1.00 0 0 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 1.00 Montevil 2015 0.99 1.00 1.00 0 0 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 1.00 Montevil 2016 0.99 0.99 0.99 0 0 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 1.00 Montevil 2017 0.99 0.99 0.99 0 0 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0 0 0 0.99 Montevil 2018 1.00 1.00 1.00 0 0 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0 0 0 1.00 Santa Bárbara: Data since the year 2016. Variables: NO, NO2, CO, PM10, 03 and PM25 barbara_data &lt;- data_completeness %&gt;% filter(station_alias == &#39;Santa Bárbara&#39;) barbara_data %&gt;% kable() %&gt;% kable_styling() station_alias year SO2 NO NO2 CO PM10 O3 dd vv TMP HR PRB RS LL BEN TOL MXIL PM25 Santa Bárbara 2016 0 0.97 0.97 0.98 0.98 0 0 0 0 0 0 0 0 0 0 0 0.98 Santa Bárbara 2017 0 0.98 0.98 0.99 1.00 0 0 0 0 0 0 0 0 0 0 0 1.00 Santa Bárbara 2018 0 1.00 1.00 1.00 1.00 0 0 0 0 0 0 0 0 0 0 0 1.00 All the stations have 2018 data, but it is just 6 observations. We drop them to avoid problems when visualising the data. observations_per_year &lt;- air_data_2 %&gt;% group_by(year = year(date_time_utc)) %&gt;% summarise(n = n()) observations_per_year %&gt;% kable() %&gt;% kable_styling() year n 2000 35136 2001 35040 2002 35040 2003 35040 2004 35136 2005 35040 2006 34939 2007 34921 2008 35136 2009 39541 2010 43800 2011 43800 2012 43920 2013 43800 2014 43800 2015 43416 2016 52703 2017 52560 2018 6 air_data_2$year &lt;- year(air_data_2$date_time_utc) air_data_2 &lt;- air_data_2 %&gt;% filter(year != &#39;2018&#39;) 7.3 Adding new variables 7.3.1 Time variables We add to the dataset several more time variables. air_data_2$month &lt;- month(air_data_2$date_time_utc) air_data_2$date &lt;- as.Date(air_data_2$date_time_utc) air_data_2$week_day &lt;- wday(air_data_2$date_time_utc, week_start = getOption(&quot;lubridate.week.start&quot;, 1)) air_data_2$hour &lt;- hour(air_data_2$date_time_utc) 7.3.2 Laboral dates And we add a variable with the with the ‘non-working days’ of Gijon city from 2014 to 2017. holydays &lt;- read_csv(&#39;data/holiday_dates.csv&#39;, locale = locale(encoding = &quot;ISO-8859-1&quot;)) air_data_2 &lt;- left_join(air_data_2, holydays, by = c(&quot;date&quot; = &quot;holiday_date&quot;)) air_data_2 &lt;- air_data_2 %&gt;% mutate(no_lab_days = ifelse((week_day &lt; 6 &amp; !is.na(holiday_type)) | (week_day &gt;=6), &quot;no_lab&quot;, &quot;lab&quot;)) %&gt;% mutate(no_lab_days=replace(no_lab_days, date &lt; &#39;2014-01-01&#39;, NA)) 7.3.3 Wind direction We create another variable to have a factor version of the ‘dd’ variable (wind direction in degrees). I took this snippet of code from here: https://community.rstudio.com/t/convert-wind-direction-degrees-into-factors-in-a-data-frame/14636/4 I made some changes because this code caused a problem when I tried to publish the document on bookdown rose_breaks &lt;- c(0, 360/32, (1/32 + (1:15 / 16)) * 360, 360) # The problem was the repetition of the level &quot;N&quot;. # So I splited this level in two, &quot;N1&quot; and &quot;N2&quot;. rose_labs &lt;- c( &quot;N1&quot;, &quot;NNE&quot;, &quot;NE&quot;, &quot;ENE&quot;, &quot;E&quot;, &quot;ES&quot;, &quot;SE&quot;, &quot;SSE&quot;, &quot;S&quot;, &quot;SSW&quot;, &quot;SW&quot;, &quot;WS&quot;, &quot;W&quot;, &quot;WNW&quot;, &quot;NW&quot;, &quot;NNW&quot;, &quot;N2&quot; ) air_data_2 &lt;- air_data_2 %&gt;% mutate( wd = cut( dd, breaks = rose_breaks, labels = rose_labs, right = FALSE, include.lowest = TRUE ) ) # And I recoded to &quot;N&quot; air_data_2 &lt;- air_data_2 %&gt;% mutate(wd = recode(wd, N1 = &quot;N&quot;, N2 = &quot;N&quot;)) We save the final dataset as a rds object. saveRDS(air_data_2, file = &quot;data_rds/air_data_2.rds&quot;) 7.3.4 Tables preparation for Tableau dashboards We are going to prepare some tables for the Tableau dashboards. First of all we export the whole table to a csv file. I am not saving this file in the “data_final_csvs” folder because it exceeds the Github file limit size (100mb)(pendiente exportar a google drive) I save the air_data_2.csv outside the project diretory because it exceeds the 100mb github limit. # write_csv(air_data_2,&quot;C:/Users/SErgio/OneDrive/00_master_data_science/TFM/air_data_2.csv&quot;) 7.3.4.1 CO tables We create a CO dataset with CO Moving averages (each 8 hours). It is needed to measure the accomplishment of UE limits for this pollutant co_data &lt;- air_data_2 %&gt;% select(station_alias, date_time_utc, CO) %&gt;% mutate(ma_co_8 = roll_mean(CO, 8, fill=0)) hist(co_data$ma_co_8) summary(co_data$ma_co_8) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 0.25 0.39 0.48 0.60 24.34 116415 plot(co_data$ma_co_8) # pending na treatment saveRDS(co_data, file = &quot;data_rds/co_data.rds&quot;) write_csv(co_data, &quot;data_final_csvs/co_data.csv&quot;) 7.3.4.2 O3 tables We create a O3 dataset with O3 Moving averages (each 8 hours). It is needed to measure the accomplishment of UE limits for this pollutant o3_data &lt;- air_data_2 %&gt;% select(station_alias, date_time_utc, O3) %&gt;% mutate(ma_o3_8 = roll_mean(O3, 8, fill=0), o3_8_acc = ifelse(ma_o3_8 &gt; 125, &#39;no&#39;, &#39;yes&#39;)) hist(o3_data$ma_o3_8) summary(o3_data$ma_o3_8) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 21.50 37.50 39.34 54.10 998.00 41435 plot(o3_data$ma_o3_8) # pending na treatment saveRDS(o3_data, file = &quot;data_rds/o3_data.rds&quot;) write_csv(o3_data, &quot;data_final_csvs/o3_data.csv&quot;) 7.3.4.3 SO2 tables RD 102/2011 Hourly limit: 350 ug/m3 (1 hour). &lt;= 24 times / year. Daily limit: 125 ug/m3 (24 hours). &lt;= 3 times / year. Alert threshold: 500 ug/m3 (3 hours). so2_data &lt;- air_data_2 %&gt;% select(station_alias, date_time_utc, SO2) saveRDS(so2_data, file = &quot;data_rds/so2_data.rds&quot;) write_csv(so2_data, &quot;data_final_csvs/so2_data.csv&quot;) # How many times was the Hourly limit exceeded during the last 18 years? so2_hourly_limit &lt;- so2_data %&gt;% group_by(station_alias, year = year(date_time_utc), day = date(date_time_utc)) %&gt;% filter(SO2 &gt; 350) %&gt;% summarise(n = n()) %&gt;% arrange(year) so2_hourly_limit ## # A tibble: 14 x 4 ## # Groups: station_alias, year [9] ## station_alias year day n ## &lt;fct&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; ## 1 Castilla 2001 2001-10-20 2 ## 2 H. Felgueroso 2001 2001-10-20 2 ## 3 Argentina 2004 2004-01-27 2 ## 4 Argentina 2004 2004-01-28 12 ## 5 Constitución 2007 2007-05-30 1 ## 6 Argentina 2008 2008-09-25 1 ## 7 Argentina 2008 2008-11-27 1 ## 8 Castilla 2008 2008-11-17 1 ## 9 Castilla 2008 2008-12-25 4 ## 10 Castilla 2008 2008-12-26 7 ## 11 Castilla 2009 2009-01-05 1 ## 12 Castilla 2011 2011-06-07 6 ## 13 Castilla 2011 2011-06-08 1 ## 14 Constitución 2011 2011-01-13 3 saveRDS(so2_hourly_limit, file = &quot;data_rds/so2_hourly_limit.rds&quot;) write_csv(so2_hourly_limit, &quot;data_final_csvs/so2_hourly_limit.csv&quot;) # How many times was the Daily limit exceeded during the last 18 years? so2_daily_limit &lt;- so2_data %&gt;% group_by(station_alias, date = date(date_time_utc), year = year(date_time_utc)) %&gt;% summarise(avg = mean(SO2, na.rm = TRUE)) %&gt;% ungroup %&gt;% filter(avg &gt; 125) %&gt;% group_by(station_alias, year, date) %&gt;% summarise(n = n()) %&gt;% arrange(year) so2_daily_limit ## # A tibble: 6 x 4 ## # Groups: station_alias, year [3] ## station_alias year date n ## &lt;fct&gt; &lt;dbl&gt; &lt;date&gt; &lt;int&gt; ## 1 Argentina 2004 2004-01-28 1 ## 2 Castilla 2008 2008-11-17 1 ## 3 Castilla 2008 2008-12-25 1 ## 4 Castilla 2008 2008-12-26 1 ## 5 Castilla 2011 2011-06-07 1 ## 6 Castilla 2011 2011-06-08 1 saveRDS(so2_daily_limit, file = &quot;data_rds/so2_daily_limit.rds&quot;) write_csv(so2_daily_limit, &quot;data_final_csvs/so2_daily_limit.csv&quot;) "],
["data-exploration.html", "8 Data Exploration 8.1 Relationships between variables 8.2 PM10 Constitucion Station", " 8 Data Exploration Loading packages library(readr) library(dplyr) library(tidyr) library(openair) # http://davidcarslaw.github.io/openair/ library(purrr) library(lubridate) library(ggplot2) library(stringr) library(knitr) library(xts) library(zoo) library(gridExtra) library(astsa) library(rvest) library(fpp2) library(ranger) library(broom) library(RcppRoll) library(reshape2) Data loading air_data_2 &lt;- readRDS(&quot;data_rds/air_data_2.rds&quot;) We take a look to the general trend of several indicators through the last 18 years # We calcule the yearly mean of the pollutants levels. year_avgs &lt;- air_data_2 %&gt;% select(station_alias, date_time_utc, PM10, PM25, SO2, NO2, NO, O3, BEN, CO, MXIL, TOL) %&gt;% group_by(station_alias, year = year(date_time_utc)) %&gt;% summarise_all(funs(mean(., na.rm = TRUE))) %&gt;% select(-date_time_utc) # We drop this variable # We export the years_avgs as a csv file write_csv(year_avgs, &quot;data_final_csvs/year_avgs.csv&quot;) # We convert the table to long format year_avgs_long &lt;- gather(year_avgs, contaminante, value, 3:length(year_avgs)) %&gt;% filter(!(station_alias == &#39;Constituci?n&#39; &amp; year == &#39;2006&#39; &amp; contaminante %in% c(&#39;BEN&#39;, &#39;MXIL&#39;, &#39;TOL&#39;))) %&gt;% # We filter this data because is only completed in 0.01% filter(!(station_alias == &#39;Constituci?n&#39; &amp; year == &#39;2008&#39; &amp; contaminante == &#39;PM25&#39;)) # We filter this data because is only completed in 0.02% # We present the data in a grid of graphs ggplot(year_avgs_long, aes(x = year, y = value)) + geom_line() + facet_grid(contaminante~station_alias,scales=&quot;free_y&quot;) + theme(axis.text = element_text(size = 6)) We drop the Santa Bárbara and Montevil stations. These stations have much less data and the behavior of their variables are significantly different (they are sub-urban stations). So, we take them out from the analysis for now. air_data_3 &lt;- air_data_2 %&gt;% filter(station_alias != &#39;Montevil&#39; , station_alias != &#39;Santa B?rbara&#39; ) # We calcule the yearly mean of the pollutants levels. year_avgs &lt;- air_data_3 %&gt;% select(station_alias, date_time_utc, PM10, PM25, SO2, NO2, NO, O3, BEN, CO, MXIL, TOL) %&gt;% group_by(station_alias, year = year(date_time_utc)) %&gt;% summarise_all(funs(mean(., na.rm = TRUE))) %&gt;% select(-date_time_utc) # quito ahora esta variable, porque no tiene sentido que salga su media. # We convert the table to long format year_avgs_long &lt;- gather(year_avgs, contaminante, value, 3:length(year_avgs)) %&gt;% filter(!(station_alias == &#39;Constituci?n&#39; &amp; year == &#39;2006&#39; &amp; contaminante %in% c(&#39;BEN&#39;, &#39;MXIL&#39;, &#39;TOL&#39;))) %&gt;% # We filter this data because is only completed in 0.01% filter(!(station_alias == &#39;Constituci?n&#39; &amp; year == &#39;2008&#39; &amp; contaminante == &#39;PM25&#39;)) # We filter this data because is only completed in 0.02% # We present the data in a grid of graphs ggplot(year_avgs_long, aes(x = year, y = value)) + geom_line() + facet_grid(contaminante~station_alias,scales=&quot;free_y&quot;) + theme(axis.text = element_text(size = 6)) # We calcule the hourly mean of the pollutants levels. hour_avgs &lt;- air_data_3 %&gt;% select(station_alias, hour, PM10, PM25, SO2, NO2, NO, O3, BEN, CO, MXIL, TOL) %&gt;% group_by(station_alias, hour) %&gt;% summarise_all(funs(mean(., na.rm = TRUE))) # quito ahora esta variable, porque no tiene sentido que salga su media. # We convert the table to long format hour_avgs_long &lt;- gather(hour_avgs, contaminante, value, 3:length(hour_avgs)) # We present the data in a grid of graphs ggplot(hour_avgs_long, aes(x = hour, y = value)) + geom_line() + facet_grid(contaminante~station_alias,scales=&quot;free_y&quot;) + theme(axis.text = element_text(size = 6)) # We calcule the monthly mean of the pollutants levels. month_avgs &lt;- air_data_3 %&gt;% select(station_alias, month, PM10, PM25, SO2, NO2, NO, O3, BEN, CO, MXIL, TOL) %&gt;% group_by(station_alias, month) %&gt;% summarise_all(funs(mean(., na.rm = TRUE))) # quito ahora esta variable, porque no tiene sentido que salga su media. # We convert the table to long format month_avgs_long &lt;- gather(month_avgs, contaminante, value, 3:length(month_avgs)) # We present the data in a grid of graphs ggplot(month_avgs_long, aes(x = month, y = value)) + geom_line() + facet_grid(contaminante~station_alias,scales=&quot;free_y&quot;) + theme(axis.text = element_text(size = 6)) # We calcule the weekly mean of the pollutants levels. week_day_avgs &lt;- air_data_3 %&gt;% select(station_alias, week_day, PM10, PM25, SO2, NO2, NO, O3, BEN, CO, MXIL, TOL) %&gt;% group_by(station_alias, week_day) %&gt;% summarise_all(funs(mean(., na.rm = TRUE))) # quito ahora esta variable, porque no tiene sentido que salga su media. # We convert the table to long format week_day_avgs_long &lt;- gather(week_day_avgs, contaminante, value, 3:length(week_day_avgs)) # We present the data in a grid of graphs ggplot(week_day_avgs_long, aes(x = week_day, y = value)) + geom_line() + facet_grid(contaminante~station_alias,scales=&quot;free_y&quot;) + theme(axis.text = element_text(size = 6)) 8.1 Relationships between variables The Constitucion Station is the only station with meteorological data. So, we are going to focus our efforts of data exploration on this station. Reference: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization #This is from the future ;) (tengo que copiar el codigo aqui) constitucion_data &lt;- readRDS(&quot;data_rds/constitucion_data.rds&quot;) pollutants &lt;- constitucion_data %&gt;% select(PM10, PM25, NO2, NO, SO2, CO, O3) %&gt;% na.omit() cor_matrix &lt;- round(cor(pollutants), 2) head(cor_matrix) ## PM10 PM25 NO2 NO SO2 CO O3 ## PM10 1.00 0.56 0.50 0.49 0.29 0.38 -0.26 ## PM25 0.56 1.00 0.37 0.37 0.21 0.29 -0.26 ## NO2 0.50 0.37 1.00 0.68 0.30 0.47 -0.59 ## NO 0.49 0.37 0.68 1.00 0.29 0.44 -0.45 ## SO2 0.29 0.21 0.30 0.29 1.00 0.28 -0.23 ## CO 0.38 0.29 0.47 0.44 0.28 1.00 -0.29 long_cor_matrix &lt;- melt(cor_matrix) head(long_cor_matrix) ## Var1 Var2 value ## 1 PM10 PM10 1.00 ## 2 PM25 PM10 0.56 ## 3 NO2 PM10 0.50 ## 4 NO PM10 0.49 ## 5 SO2 PM10 0.29 ## 6 CO PM10 0.38 ggplot(data = long_cor_matrix, aes(x=Var1, y=Var2, fill=value)) + geom_tile() # Get lower triangle of the correlation matrix get_lower_tri&lt;-function(cor_matrix){ cor_matrix[upper.tri(cor_matrix)] &lt;- NA return(cor_matrix) } # Get upper triangle of the correlation matrix get_upper_tri &lt;- function(cor_matrix){ cor_matrix[lower.tri(cor_matrix)]&lt;- NA return(cor_matrix) } upper_tri &lt;- get_upper_tri(cor_matrix) upper_tri ## PM10 PM25 NO2 NO SO2 CO O3 ## PM10 1 0.56 0.50 0.49 0.29 0.38 -0.26 ## PM25 NA 1.00 0.37 0.37 0.21 0.29 -0.26 ## NO2 NA NA 1.00 0.68 0.30 0.47 -0.59 ## NO NA NA NA 1.00 0.29 0.44 -0.45 ## SO2 NA NA NA NA 1.00 0.28 -0.23 ## CO NA NA NA NA NA 1.00 -0.29 ## O3 NA NA NA NA NA NA 1.00 # Melt the correlation matrix library(reshape2) long_cor_matrix &lt;- melt(upper_tri, na.rm = TRUE) # Heatmap library(ggplot2) ggplot(data = long_cor_matrix, aes(Var2, Var1, fill = value))+ geom_tile(color = &quot;white&quot;)+ scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, midpoint = 0, limit = c(-1,1), space = &quot;Lab&quot;, name=&quot;Pearson\\nCorrelation&quot;) + theme_minimal()+ theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+ coord_fixed() reorder_cor_matrix &lt;- function(cor_matrix){ # Use correlation between variables as distance dd &lt;- as.dist((1-cor_matrix)/2) hc &lt;- hclust(dd) cor_matrix &lt;-cor_matrix[hc$order, hc$order] } # Reorder the correlation matrix cor_matrix &lt;- reorder_cor_matrix(cor_matrix) upper_tri &lt;- get_upper_tri(cor_matrix) # Melt the correlation matrix long_cor_matrix &lt;- melt(upper_tri, na.rm = TRUE) # Create a ggheatmap ggheatmap &lt;- ggplot(long_cor_matrix, aes(Var2, Var1, fill = value))+ geom_tile(color = &quot;white&quot;)+ scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, midpoint = 0, limit = c(-1,1), space = &quot;Lab&quot;, name=&quot;Pearson\\nCorrelation&quot;) + theme_minimal()+ # minimal theme theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+ coord_fixed() # Print the heatmap print(ggheatmap) ggheatmap + geom_text(aes(Var2, Var1, label = value), color = &quot;black&quot;, size = 4) + theme( axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major = element_blank(), panel.border = element_blank(), panel.background = element_blank(), axis.ticks = element_blank(), legend.justification = c(1, 0), legend.position = c(0.6, 0.7), legend.direction = &quot;horizontal&quot;)+ guides(fill = guide_colorbar(barwidth = 7, barheight = 1, title.position = &quot;top&quot;, title.hjust = 0.5)) Including information about force and direction of the wind #This is from the future ;) (tengo que copiar el codigo aqui) constitucion_data &lt;- air_data_2 %&gt;% filter(station == &quot;1&quot;) pollutants &lt;- constitucion_data %&gt;% select(PM10, PM25, NO2, NO, SO2, CO, O3, dd, vv, TMP, PRB, LL, HR, RS) %&gt;% na.omit() cor_matrix &lt;- round(cor(pollutants), 2) long_cor_matrix &lt;- melt(cor_matrix) head(long_cor_matrix) ## Var1 Var2 value ## 1 PM10 PM10 1.00 ## 2 PM25 PM10 0.56 ## 3 NO2 PM10 0.50 ## 4 NO PM10 0.49 ## 5 SO2 PM10 0.29 ## 6 CO PM10 0.38 # Get lower triangle of the correlation matrix get_lower_tri&lt;-function(cor_matrix){ cor_matrix[upper.tri(cor_matrix)] &lt;- NA return(cor_matrix) } # Get upper triangle of the correlation matrix get_upper_tri &lt;- function(cor_matrix){ cor_matrix[lower.tri(cor_matrix)]&lt;- NA return(cor_matrix) } upper_tri &lt;- get_upper_tri(cor_matrix) upper_tri ## PM10 PM25 NO2 NO SO2 CO O3 dd vv TMP PRB LL ## PM10 1 0.56 0.50 0.49 0.29 0.38 -0.26 0.01 -0.14 0.03 0.00 -0.09 ## PM25 NA 1.00 0.37 0.37 0.21 0.29 -0.26 0.02 -0.18 -0.06 0.04 -0.07 ## NO2 NA NA 1.00 0.68 0.30 0.47 -0.59 0.13 -0.35 -0.23 -0.09 0.01 ## NO NA NA NA 1.00 0.29 0.44 -0.45 0.09 -0.23 -0.17 -0.03 -0.03 ## SO2 NA NA NA NA 1.00 0.28 -0.23 0.11 -0.10 -0.05 -0.01 -0.03 ## CO NA NA NA NA NA 1.00 -0.29 0.07 -0.17 -0.15 -0.04 0.01 ## O3 NA NA NA NA NA NA 1.00 -0.30 0.61 0.27 -0.06 0.02 ## dd NA NA NA NA NA NA NA 1.00 -0.35 -0.23 0.02 0.04 ## vv NA NA NA NA NA NA NA NA 1.00 0.24 -0.12 0.03 ## TMP NA NA NA NA NA NA NA NA NA 1.00 -0.03 -0.12 ## PRB NA NA NA NA NA NA NA NA NA NA 1.00 -0.13 ## LL NA NA NA NA NA NA NA NA NA NA NA 1.00 ## HR NA NA NA NA NA NA NA NA NA NA NA NA ## RS NA NA NA NA NA NA NA NA NA NA NA NA ## HR RS ## PM10 0.06 0.00 ## PM25 0.08 -0.02 ## NO2 0.04 -0.19 ## NO 0.01 -0.08 ## SO2 -0.12 0.04 ## CO 0.10 -0.11 ## O3 -0.20 0.35 ## dd 0.02 -0.25 ## vv -0.30 0.53 ## TMP -0.15 0.39 ## PRB 0.01 0.06 ## LL 0.15 -0.08 ## HR 1.00 -0.36 ## RS NA 1.00 reorder_cor_matrix &lt;- function(cor_matrix){ # Use correlation between variables as distance dd &lt;- as.dist((1-cor_matrix)/2) hc &lt;- hclust(dd) cor_matrix &lt;-cor_matrix[hc$order, hc$order] } # Reorder the correlation matrix cor_matrix &lt;- reorder_cor_matrix(cor_matrix) upper_tri &lt;- get_upper_tri(cor_matrix) # Melt the correlation matrix long_cor_matrix &lt;- melt(upper_tri, na.rm = TRUE) # Create a ggheatmap ggheatmap &lt;- ggplot(long_cor_matrix, aes(Var2, Var1, fill = value))+ geom_tile(color = &quot;white&quot;)+ scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, midpoint = 0, limit = c(-1,1), space = &quot;Lab&quot;, name=&quot;Pearson\\nCorrelation&quot;) + theme_minimal()+ # minimal theme theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1))+ coord_fixed() ggheatmap + geom_text(aes(Var2, Var1, label = value), color = &quot;black&quot;, size = 4) + theme( axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major = element_blank(), panel.border = element_blank(), panel.background = element_blank(), axis.ticks = element_blank(), legend.justification = c(1, 0), legend.position = c(0.6, 0.7), legend.direction = &quot;horizontal&quot;)+ guides(fill = guide_colorbar(barwidth = 7, barheight = 1, title.position = &quot;top&quot;, title.hjust = 0.5)) 8.2 PM10 Constitucion Station We create the dataset pm10 with PM10 values from the Constitución Station and we execute a summary pm10 &lt;- air_data_3 %&gt;% filter(station_alias == &#39;Constituci?n&#39;) %&gt;% select(date_time_utc, PM10) summary(pm10) ## date_time_utc PM10 ## Min. :NA Min. : NA ## 1st Qu.:NA 1st Qu.: NA ## Median :NA Median : NA ## Mean :NA Mean :NaN ## 3rd Qu.:NA 3rd Qu.: NA ## Max. :NA Max. : NA 25% of the values are between 44.00 and 888.00. 888.00 is a value really extreme. How many extreme values (outliers) do we have in this series? We plot all the values to visualise this: ggplot(pm10, aes(x = date_time_utc, y = PM10)) + geom_point(alpha = 0.1) We have very few values greater than 250. So, it doesn’t seem we have a problem with the outliers (Pending: A PM10 level of 880 is something possible or is it likely to be a monitoring error?). Daily averages We create a new dataset with the PM10 daily averages and we plot them in a new graphic. We add a trend line too. There is a clear downward trend in the measurements and we have many fewer extreme values during the last decade. It seems like we have two very clear “epochs” in the data, before and after the year 2008. pm10_day_avg &lt;- pm10 %&gt;% group_by(day = date(date_time_utc)) %&gt;% summarise(day_avg = mean(PM10, na.rm = TRUE)) ggplot(pm10_day_avg, aes(x = day, y = day_avg, , colour = day_avg)) + geom_point(alpha = 0.5) + geom_smooth(color = &quot;grey&quot;, alpha = 0.2) + scale_colour_gradientn(colours = terrain.colors(10)) + theme(legend.position = c(0.3, 0.9), legend.background = element_rect(colour = &quot;transparent&quot;, fill = NA), legend.direction = &quot;horizontal&quot;) + labs(colour = &quot;PM10 daily average (colour scale)&quot;, x = &quot;Year&quot;, y = &quot;PM10 daily average&quot;, title = &quot;PM10 daily average - 2000-2017 evolution (Constitución Station)&quot;) We identify a very clear trend through the years on the last graph. But, as we already saw before on the grid graphs there are other things happening at the same time. year_const &lt;- year_avgs_long %&gt;% filter(station_alias == &quot;Constitución&quot;, contaminante == &#39;PM10&#39;) plot1 &lt;- ggplot(year_const, aes(x = year, y = value)) + geom_line() month_const &lt;- month_avgs_long %&gt;% filter(station_alias == &quot;Constitución&quot;, contaminante == &#39;PM10&#39;) plot2 &lt;- ggplot(month_const, aes(x = month, y = value)) + geom_line() week_day_const &lt;- week_day_avgs_long %&gt;% filter(station_alias == &quot;Constitución&quot;, contaminante == &#39;PM10&#39;) plot3 &lt;- ggplot(week_day_const, aes(x = week_day, y = value)) + geom_line() hour_const &lt;- hour_avgs_long %&gt;% filter(station_alias == &quot;Constitución&quot;, contaminante == &#39;PM10&#39;) plot4 &lt;- ggplot(hour_const, aes(x = hour, y = value)) + geom_line() grid.arrange(plot1, plot2, plot3, plot4, ncol = 2) "],
["prediction-models-arima.html", "9 Prediction Models. ARIMA 9.1 PM10 hourly prediction models 9.2 We have been using a dataset for training of 3 years. Is this necessary? Does the Arima algorithm take advantage of a so long time series?", " 9 Prediction Models. ARIMA On this notebook we are going to try to predict one hour - six hours ahead with ARIMA PM10 NO2 9.1 PM10 hourly prediction models We are going to focus on the Constitucion station, which is the only station w e have meteorological data. And, at first, we are going to construct models only for predicting PM10 levels. As first approach we are going to use auto regressive models. So, we will use as explicative variables only lagged values of the variable to predict. In this case PM10 values. Loading packages library(readr) library(dplyr) library(tidyr) library(openair) # http://davidcarslaw.github.io/openair/ library(purrr) library(lubridate) library(ggplot2) library(stringr) library(knitr) library(xts) library(zoo) library(gridExtra) library(astsa) library(rvest) library(fpp2) library(ranger) library(broom) library(RcppRoll) library(caret) 9.1.1 Data loading air_data_2 &lt;- readRDS(&quot;data_rds/air_data_2.rds&quot;) constitucion_data &lt;- air_data_2 %&gt;% filter(station == &quot;1&quot;) 9.1.2 Train, test and validation data We have 18 years of avalaible data. But we are not going to use all the data. Initially we are going to use just three years for the training of the models (2014-2016) and one year (2017) for testing and validation. Reasons (pending redaction): Time of training process. Not all models can take advantage of such long time series. In fact we will see as sometimes smaller datasets achieve similar results. Train data: 2014-01-01 - 2016-12-31 Test data: 2017-01-01 - 2017-09-30 Validation data: 2017-10-01 - 2017-12-31 train_201401_201612 &lt;- constitucion_data %&gt;% filter(date_time_utc &gt;= &#39;2014-01-01 00:00:00&#39;, date_time_utc &lt;= &#39;2016-12-31 23:00:00&#39;) %&gt;% select(PM10) %&gt;% mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %&gt;% # replacing NAs by the mean. rename(PM10_0 = PM10) %&gt;% ts(frequency = 24) # We generate a smaller training dataset with the last three months of 2016 train_201610_2016_12 &lt;- constitucion_data %&gt;% filter(date_time_utc &gt;= &#39;2016-10-01 00:00:00&#39;, date_time_utc &lt;= &#39;2016-12-31 23:00:00&#39;) %&gt;% select(PM10) %&gt;% mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %&gt;% rename(PM10_0 = PM10) %&gt;% ts(frequency = 24) test_201701_201709 &lt;- constitucion_data %&gt;% filter(date_time_utc &gt;= &#39;2017-01-01 00:00:00&#39;, date_time_utc &lt;= &#39;2017-09-30 23:00:00&#39;) %&gt;% select(PM10) %&gt;% mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %&gt;% rename(PM10_0 = PM10) %&gt;% ts(frequency = 24) # We generate a smaller testing dataset with the first two weeks of 2016. test_20170101_20170114 &lt;- constitucion_data %&gt;% filter(date_time_utc &gt;= &#39;2017-01-01 00:00:00&#39;, date_time_utc &lt;= &#39;2017-01-14 23:00:00&#39;) %&gt;% select(PM10) %&gt;% mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %&gt;% rename(PM10_0 = PM10) %&gt;% ts(frequency = 24) validation_201710_201712 &lt;- constitucion_data %&gt;% filter(date_time_utc &gt;= &#39;2017-10-01 00:00:00&#39;, date_time_utc &lt;= &#39;2017-12-31 23:00:00&#39;) %&gt;% select(PM10) %&gt;% mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) %&gt;% rename(PM10_0 = PM10) %&gt;% ts(frequency = 24) 9.1.3 Data Exploration we plot the three time series using the function autoplot we just generated. autoplot(train_201401_201612) autoplot(test_201701_201709, ylim = c(0, 400)) autoplot(validation_201710_201712,ylim = c(0, 400)) ### Base model. In order to create a base model we are going to take as prediction the value from the previous hour, forecasting just one hour ahead. # As we don&#39;t need any training, because we have already defined the model as Xt = Xt-1, we only need to create a testing dataset. We choose the same period as before but we don&#39;t use the time series format (ts). test_201701_201709_2 &lt;- constitucion_data %&gt;% filter(date_time_utc &gt;= &#39;2017-01-01 00:00:00&#39;, date_time_utc &lt; &#39;2017-10-01 00:00:00&#39;) %&gt;% select(PM10) %&gt;% mutate(PM10 = replace_na(PM10, mean(PM10, na.rm = TRUE))) # We replace the nas (36) by the mean. base_model &lt;- test_201701_201709_2 %&gt;% mutate(y_pred = lag(PM10, 1)) %&gt;% # We create the column y_pred with the lagged value of PM10 (one hour). rename(y_test = PM10) %&gt;% # We change the name of the PM10 column to y_test na.omit() # We remove the observations with nas (just the first row) y_test &lt;- base_model$y_test y_pred &lt;- base_model$y_pred sd_test &lt;- sd(y_test) RMSE &lt;- RMSE(y_test, y_pred) MAE &lt;- MAE(y_test, y_pred) rss &lt;- sum((y_pred - y_test) ** 2) tss &lt;- sum((y_test - mean(y_test)) ** 2) R_squared &lt;- 1 - rss/tss paste(c(&quot;R-Squared:&quot;, round(R_squared, 4), &quot;MAE:&quot;, round(MAE, 2), &quot;RMSE:&quot;, round(RMSE, 2), &quot;Standard Deviation (test data):&quot;, round(sd_test, 2))) ## [1] &quot;R-Squared:&quot; &quot;0.5327&quot; ## [3] &quot;MAE:&quot; &quot;5.15&quot; ## [5] &quot;RMSE:&quot; &quot;7.45&quot; ## [7] &quot;Standard Deviation (test data):&quot; &quot;10.9&quot; So, for the one hour PM10 prediction if we take as prediction the previous PM10 hour level we would have a R-squared of 0.5327. So, the model is explaining 53% of the variability of the target variable. Estimation of errors. We are going to use the MAE (Mean Absolute Error) in order to compare the different models. We choose the MAE over the RMSE (Root Mean Squared Error) because it is easier to interpret and at the same time it is more robust (less sensitive to outliers). Either way, we are going to calculate the RMSE too, precisely because its sensitivity to outliers. It will give us useful information about the behaviour of each model. We take a look to the relation between the predictions and the actual values. df &lt;- bind_cols(as.data.frame(y_pred), as.data.frame(y_test)) y_pred_y_test_base_model_graph &lt;- ggplot(data = df, aes(x = y_test, y = y_pred)) + geom_point(alpha = 0.5) + theme_minimal() y_pred_y_test_base_model_graph We plot the distribution of the errors ggplot(data = df, aes(x = y_pred - y_test)) + geom_histogram() + theme_minimal() We plot two lines with the predictions and the actual values. We have a problem of overplotting. autoplot(ts(base_model$y_pred), series=&quot;1-step fitted values&quot;) + autolayer(ts(base_model$y_test), series=&quot;Test data&quot;) + theme_minimal() + theme(legend.position=&quot;top&quot;) We plot just the first 14 days of 2017. two_weeks &lt;- base_model %&gt;% slice(1:336) # 336 corresponds to 24 hours * 14 days. autoplot(ts(two_weeks$y_pred), series=&quot;1-step fitted values&quot;) + autolayer(ts(two_weeks$y_test), series=&quot;Test data&quot;) + theme_minimal() + theme(legend.position=&quot;top&quot;) And as expected, we get two identical lines. But one of them, the prediction, one step forward than the line with the actual data. 9.1.4 ARIMA model To fit our first ARIMA model we are going to use the auto.arima function from the Forecast package. This function … # model_arima_0 = auto.arima(train_201401_201612,seasonal=TRUE,trace=TRUE) # It took 2 hours to fit the model # saveRDS(model_arima_0, &quot;data_rds/model_arima_0.rds&quot;) model_arima_0 &lt;- readRDS(&quot;data_rds/model_arima_0.rds&quot;) summary(model_arima_0) ## Series: train_201401_201612 ## ARIMA(2,1,3)(0,0,2)[24] ## ## Coefficients: ## ar1 ar2 ma1 ma2 ma3 sma1 sma2 ## 0.0991 0.5519 -0.5228 -0.6175 0.1642 0.0564 0.0392 ## s.e. NaN NaN NaN NaN NaN 0.0063 0.0061 ## ## sigma^2 estimated as 87.28: log likelihood=-96095.71 ## AIC=192207.4 AICc=192207.4 BIC=192272.8 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 0.003353175 9.341125 5.774614 -Inf Inf 0.5111664 0.002213004 #test$arima=forecast(model_arima,h=24)$mean EXPLAIN THE RESULTS p, d, q P, D, Q etc… To obtain the R-Squared… y_train &lt;- forecast(model_arima_0, h = 1)$fitted # We make a forecast one hour ahead and we extract the fitted values from the object forecast. x_train &lt;- train_201401_201612 rss &lt;- sum((y_train - x_train) ** 2) tss &lt;- sum((x_train - mean(x_train)) ** 2) R_squared &lt;- 1 - rss/tss paste(c(&quot;R-Squared:&quot;, round(R_squared, 4))) ## [1] &quot;R-Squared:&quot; &quot;0.6031&quot; The R-Squared has increased from 0.5327 to 0.6031. But both, RMSE (9.34) and MAE (5.77), are greater than the scores obtained with the Base Model. But we are comparing two different periods, the testing period (Base Model; 2017-01 - 2017-09) versus the training period (ARIMA model; 2014 - 2016). To make a fair comparison we will have to compare the same periods. Especially since we know they have important differences in variability. For this we are going to apply the ARIMA model model_arima_0 on the testing data. Doing so we will be able to get the fitted values from this period and we will be able to obtain its error scores. y_pred &lt;- forecast(test_201701_201709, model = model_arima_0, h=1)$fitted # We call the forecast function passing the testing data through the ARIMA model fitted with the training data and we extract the fitted values. y_test &lt;- test_201701_201709 sd_test &lt;- sd(y_test) RMSE &lt;- RMSE(y_test, y_pred) MAE &lt;- MAE(y_test, y_pred) rss &lt;- sum((y_pred - y_test) ** 2) tss &lt;- sum((y_test - mean(y_test)) ** 2) R_squared &lt;- 1 - rss/tss paste(c(&quot;R-Squared:&quot;, round(R_squared, 4), &quot;MAE:&quot;, round(MAE, 2), &quot;RMSE:&quot;, round(RMSE, 2), &quot;Standard Deviation (test data):&quot;, round(sd_test, 2))) ## [1] &quot;R-Squared:&quot; &quot;0.6015&quot; ## [3] &quot;MAE:&quot; &quot;4.84&quot; ## [5] &quot;RMSE:&quot; &quot;6.88&quot; ## [7] &quot;Standard Deviation (test data):&quot; &quot;10.9&quot; We confirm we are obtaining a better R-Squared: 0.6015 versus the 0.5327 obtained by the Base Model. The errors are sustantially better too. The ARIMA model obtains a MAE of 4.84 (Base Model: 5.15) and a RMSE of 6.88 (Base Model: 7.45). Furthermore, the prediction errors on the test data are smaller than in the training data. It could be counterintuitive but the test period (2017-01 - 2017-09) contains much less variability (sd = 10.91) than the training period (2014-01 - 2016-12; sd = 14.83). This could be affecting the error scores. We plot the fitted values and the test data. autoplot(y_test, series=&quot;Test data&quot;) + autolayer(y_pred, series=&quot;1-step fitted values&quot;) + theme_minimal() + theme(legend.position=&quot;top&quot;) We have made the forecast for one hour ahead and we have estimated its precission. But what happens when we try to forecast more distant values? In order to check the precission of this ARIMA model with forecasts more … we are going to use the fitted # First of all we create with the Arima function the object arima.test arima.test &lt;- Arima(test_201701_201709, model=model_arima_0) And with the fitted function, setting its parameter to 6, we fit and extract the fitted values corresponding to the forecast 6 hours ahead (check redaction) h6 &lt;- fitted(arima.test, h = 6) # It takes a lot of time saveRDS(h6, &quot;data_rds/arima_model_0_test_fitted_values_h6.rds&quot;) # We save the final object as a rds file fitted_values_h6 &lt;- readRDS(&quot;data_rds/arima_model_0_test_fitted_values_h6.rds&quot;) We assign to y_test and y_pred the test dataset and its predictions. y_test &lt;- test_201701_201709 y_pred &lt;- fitted_values_h6 The first rows of y_pred are NAs. That is because we are making prediction 6 hours ahead. head(y_pred, 10) ## Time Series: ## Start = c(1, 1) ## End = c(1, 10) ## Frequency = 24 ## x ## [1,] NA ## [2,] NA ## [3,] NA ## [4,] NA ## [5,] NA ## [6,] NA ## [7,] NA ## [8,] 31.67340 ## [9,] 36.69984 ## [10,] 36.44485 So, we remove the NAs from the y_pred time series, and the equivalent rows from the y_test object. y_pred &lt;- y_pred %&gt;% na.omit() y_test &lt;- y_test %&gt;% subset(start = 8) And we obtain the scores to know the goodness of the model of 6 hours ahead RMSE &lt;- RMSE(y_test, y_pred) MAE &lt;- MAE(y_test, y_pred) rss &lt;- sum((y_pred - y_test) ** 2) tss &lt;- sum((y_test - mean(y_test)) ** 2) R_squared &lt;- 1 - rss/tss paste(c(&quot;R-Squared:&quot;, round(R_squared, 4), &quot;MAE:&quot;, round(MAE, 2), &quot;RMSE:&quot;, round(RMSE, 2), &quot;Standard Deviation (test data):&quot;, round(sd_test, 2))) ## [1] &quot;R-Squared:&quot; &quot;0.2118&quot; ## [3] &quot;MAE:&quot; &quot;7.12&quot; ## [5] &quot;RMSE:&quot; &quot;9.68&quot; ## [7] &quot;Standard Deviation (test data):&quot; &quot;10.9&quot; R-Squared: 0.2118. When we try to predict the levels of PM10 6 hours ahead this Arima model is only able to explain a 21% of the variability. Errors: Consequentely the errors grow too… 9.2 We have been using a dataset for training of 3 years. Is this necessary? Does the Arima algorithm take advantage of a so long time series? To check this we are going to use now a training period much shorter. We will use just three months, the last three months of 2016. We repeat … model_arima_1 = auto.arima(train_201610_2016_12,seasonal=TRUE,trace=TRUE) # saveRDS(model_arima_1, &quot;data_rds/model_arima_1.rds&quot;) model_arima_1 &lt;- readRDS(&quot;data_rds/model_arima_1.rds&quot;) summary(model_arima_1) ## Series: train_lite ## ARIMA(2,1,4)(0,0,2)[24] ## ## Coefficients: ## ar1 ar2 ma1 ma2 ma3 ma4 sma1 sma2 ## 0.1006 0.547 -0.5222 -0.6079 0.1543 -0.0107 0.0977 0.1135 ## s.e. NaN NaN NaN NaN NaN 0.0184 0.0222 0.0215 ## ## sigma^2 estimated as 71.56: log likelihood=-7844.97 ## AIC=15707.94 AICc=15708.02 BIC=15759.24 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 0.1536203 8.442183 5.17907 -Inf Inf 0.545021 -0.0004040565 The model selected by the auto.arima function is very similar to the former model train_201610_2016_12: ARIMA(2,1,4)(0,0,2)[24] train_201401_201612: ARIMA(2,1,3)(0,0,2)[24] Coefficients lite: ar1 ar2 ma1 ma2 ma3 ma4 sma1 sma2 0.1006 0.547 -0.5222 -0.6079 0.1543 -0.0107 0.0977 0.1135 Coefficients 2014-2016: ar1 ar2 ma1 ma2 ma3 sma1 sma2 0.0991 0.5519 -0.5228 -0.6175 0.1642 0.0564 0.0392 We apply this model to the testing data y_pred &lt;- forecast(test_201701_201709, model = model_arima_1, h=1)$fitted # We call the forecast function passing the testing data through the ARIMA model fitted with the training data and we extract the fitted values. y_test &lt;- test_201701_201709 sd_test &lt;- sd(y_test) RMSE &lt;- RMSE(y_test, y_pred) MAE &lt;- MAE(y_test, y_pred) rss &lt;- sum((y_pred - y_test) ** 2) tss &lt;- sum((y_test - mean(y_test)) ** 2) R_squared &lt;- 1 - rss/tss paste(c(&quot;R-Squared:&quot;, round(R_squared, 4), &quot;MAE:&quot;, round(MAE, 2), &quot;RMSE:&quot;, round(RMSE, 2), &quot;Standard Deviation (test data):&quot;, round(sd_test, 2))) ## [1] &quot;R-Squared:&quot; &quot;0.5975&quot; ## [3] &quot;MAE:&quot; &quot;4.88&quot; ## [5] &quot;RMSE:&quot; &quot;6.92&quot; ## [7] &quot;Standard Deviation (test data):&quot; &quot;10.9&quot; The model scores barely changes. So, in order to speed computation we are going to use this training period from now. If we included seasonal variables as the month or the year we would have to use more long datasets. test lite y_pred &lt;- forecast(test_20170101_20170114, model = model_arima_0, h=1)$fitted y_test &lt;- test_20170101_20170114 sd_test &lt;- sd(y_test) RMSE &lt;- RMSE(y_test, y_pred) MAE &lt;- MAE(y_test, y_pred) rss &lt;- sum((y_pred - y_test) ** 2) tss &lt;- sum((y_test - mean(y_test)) ** 2) R_squared &lt;- 1 - rss/tss paste(c(&quot;R-Squared:&quot;, round(R_squared, 4), &quot;MAE:&quot;, round(MAE, 2), &quot;RMSE:&quot;, round(RMSE, 2), &quot;Standard Deviation (test data):&quot;, round(sd_test, 2))) ## [1] &quot;R-Squared:&quot; &quot;0.6849&quot; ## [3] &quot;MAE:&quot; &quot;5.71&quot; ## [5] &quot;RMSE:&quot; &quot;7.98&quot; ## [7] &quot;Standard Deviation (test data):&quot; &quot;14.24&quot; autoplot(y_test, series=&quot;Test data&quot;) + autolayer(y_pred, series=&quot;1-step fitted values&quot;) + theme(legend.position=&quot;top&quot;) arima.test_20170101_20170114 &lt;- Arima(test_20170101_20170114, model=model_arima_1) accuracy(arima.test) ## ME RMSE MAE MPE MAPE MASE ## Training set -0.01886331 6.883032 4.837916 -15.72006 32.48397 0.5045182 ## ACF1 ## Training set 0.04597431 h1_lite &lt;- fitted(arima.test_20170101_20170114, h = 1) h2_lite &lt;- fitted(arima.test_20170101_20170114, h = 2) h6_lite &lt;- fitted(arima.test_20170101_20170114, h = 6) autoplot(y_test, series=&quot;Test data&quot;) + autolayer(h6_lite, series=&quot;6-step fitted values&quot;) + theme(legend.position=&quot;top&quot;) autoplot(y_test, series=&quot;Test data&quot;) + autolayer(h1_lite, series=&quot;1-step fitted values&quot;) + theme(legend.position=&quot;top&quot;) 9.2.1 NO2 Predictions train_201610_2016_12 &lt;- constitucion_data %&gt;% filter(date_time_utc &gt;= &#39;2016-10-01 00:00:00&#39;, date_time_utc &lt;= &#39;2016-12-31 23:00:00&#39;) %&gt;% select(NO2) %&gt;% mutate(NO2 = replace_na(NO2, mean(NO2, na.rm = TRUE))) %&gt;% rename(NO2_0 = NO2) %&gt;% ts(frequency = 24) test_201701_201709 &lt;- constitucion_data %&gt;% filter(date_time_utc &gt;= &#39;2017-01-01 00:00:00&#39;, date_time_utc &lt;= &#39;2017-09-30 23:00:00&#39;) %&gt;% select(NO2) %&gt;% mutate(NO2 = replace_na(NO2, mean(NO2, na.rm = TRUE))) %&gt;% rename(NO2_0 = NO2) %&gt;% ts(frequency = 24) # We generate a testing dataset with the first two weeks of 2016. test_20170101_20170114 &lt;- constitucion_data %&gt;% filter(date_time_utc &gt;= &#39;2017-01-01 00:00:00&#39;, date_time_utc &lt;= &#39;2017-01-14 23:00:00&#39;) %&gt;% select(NO2) %&gt;% mutate(NO2 = replace_na(NO2, mean(NO2, na.rm = TRUE))) %&gt;% rename(NO2_0 = NO2) %&gt;% ts(frequency = 24) validation_201710_201712 &lt;- constitucion_data %&gt;% filter(date_time_utc &gt;= &#39;2017-10-01 00:00:00&#39;, date_time_utc &lt;= &#39;2017-12-31 23:00:00&#39;) %&gt;% select(NO2) %&gt;% mutate(NO2 = replace_na(NO2, mean(NO2, na.rm = TRUE))) %&gt;% rename(NO2_0 = NO2) %&gt;% ts(frequency = 24) This time we are going straight to create an ARIMA model. 9.2.2 ARIMA model Using the train_201610_2016_12 NO2_model_arima_1 = auto.arima(train_201610_2016_12,seasonal=TRUE,trace=TRUE) saveRDS(NO2_model_arima_1, &quot;data_rds/NO2_model_arima_1.rds&quot;) NO2_model_arima_1 &lt;- readRDS(&quot;data_rds/NO2_model_arima_1.rds&quot;) summary(NO2_model_arima_1) ## Series: train_lite ## ARIMA(1,0,0)(2,1,0)[24] ## ## Coefficients: ## ar1 sar1 sar2 ## 0.7979 -0.6270 -0.3073 ## s.e. 0.0129 0.0204 0.0203 ## ## sigma^2 estimated as 70.45: log likelihood=-7753.33 ## AIC=15514.65 AICc=15514.67 BIC=15537.41 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.07054555 8.341781 6.09835 -5.95093 21.65569 0.4987294 ## ACF1 ## Training set 0.0299381 We apply this model to the testing data y_pred &lt;- forecast(test_201701_201709, model = NO2_model_arima_1, h=1)$fitted # We call the forecast function passing the testing data through the ARIMA model fitted with the training data and we extract the fitted values. y_test &lt;- test_201701_201709 sd_test &lt;- sd(y_test) RMSE &lt;- RMSE(y_test, y_pred) MAE &lt;- MAE(y_test, y_pred) rss &lt;- sum((y_pred - y_test) ** 2) tss &lt;- sum((y_test - mean(y_test)) ** 2) R_squared &lt;- 1 - rss/tss paste(c(&quot;R-Squared:&quot;, round(R_squared, 4), &quot;MAE:&quot;, round(MAE, 2), &quot;RMSE:&quot;, round(RMSE, 2), &quot;Standard Deviation (test data):&quot;, round(sd_test, 2))) ## [1] &quot;R-Squared:&quot; &quot;0.7175&quot; ## [3] &quot;MAE:&quot; &quot;6.66&quot; ## [5] &quot;RMSE:&quot; &quot;9.84&quot; ## [7] &quot;Standard Deviation (test data):&quot; &quot;18.51&quot; Better R-Squared than the PM10 model. But worse errors scores. Different units. Maybe we can’t compare. More variability. Explain this. Include graphics. "],
["python-scripts.html", "Python scripts", " Python scripts "]
]
